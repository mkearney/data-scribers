---
title: "Approximating the KL Divergence Between Implicit Distributions with Density Ratio Estimation"
author: 'tiao.io'
date: '2018-08-27'
slug: approximating-the-kl-divergenc
link: https://tiao.io/post/approximating-the-kl-divergence-between-implicit-distribution-with-density-ratio-estimation/
categories:
- bloglink
tags:
  - rstats
  - machine-learning
  - modeling
  - tiaoio
---

The Kullback-Leibler (KL) divergence between distributions $p$ and $q$ is defined as $$ \mathcal{D}_{\mathrm{KL}}[p(x) || q(x)] := \mathbb{E}_{p(x)} \left [ \log \left ( \frac{p(x)}{q(x)} \right )[... <i class="fas fa-external-link-alt"></i>](https://tiao.io/post/approximating-the-kl-divergence-between-implicit-distribution-with-density-ratio-estimation/)

