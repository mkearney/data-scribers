<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linear Model on rscribers</title>
    <link>/categories/linear-model/</link>
    <description>Recent content in Linear Model on rscribers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Sep 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/linear-model/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Data Science in Mental Health</title>
      <link>/post/data-science-in-mental-health/</link>
      <pubDate>Thu, 20 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/data-science-in-mental-health/</guid>
      <description>I came across two articles recently that I thought spoke to each other in an interesting way. The first was a New York Times piece about the failings of data science firms who try to identify school shootings before they happen by social media&amp;hellip;</description>
    </item>
    
    <item>
      <title>Visualzing Results of German Elections (2013 and 2017)</title>
      <link>/post/visualzing-results-of-german-e/</link>
      <pubDate>Wed, 19 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/visualzing-results-of-german-e/</guid>
      <description>Data Sources: https://www.govdata.de/web/guest/apps/-/details/bundestagswahl-2017 https://www.bundeswahlleiter.de/bundestagswahlen/2017/strukturdaten.html Settings, Packages and&amp;hellip;</description>
    </item>
    
    <item>
      <title>Introducing debkeepr</title>
      <link>/post/introducing-debkeepr/</link>
      <pubDate>Tue, 18 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/introducing-debkeepr/</guid>
      <description>After an extensive period of iteration and a long but rewarding process of learning about package development, I am pleased to announce the release of my first R package. The package is called debkeepr , and it derives directly from my historical research on early modern merchants . debkeepr provides an interface for working with non-decimal currencies that use the tripartite system of pounds, shillings, and pence that was used throughout Europe in the medieval and early modern&amp;hellip;</description>
    </item>
    
    <item>
      <title>Tick marks, variable names, and ggplot2</title>
      <link>/post/tick-marks-variable-names-and-/</link>
      <pubDate>Mon, 17 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tick-marks-variable-names-and-/</guid>
      <description>A popular workflow in R uses {dplyr} to group_by() and then summarise() 1 variables. It&amp;rsquo;s an intuitive and easy way to aggregate and describe data, especially along multiple dimensions. The cost of being both powerful and user-friendly, however, is its arguably inconvenient default method for assigning names to summarized values. As the code illustrates below, users can provide their own names when using&amp;hellip;</description>
    </item>
    
    <item>
      <title>Binary, beta, beta-binomial</title>
      <link>/post/binary-beta-betabinomial/</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/binary-beta-betabinomial/</guid>
      <description>I’ve been working on updates for the simstudy package. In the past few weeks, a couple of folks independently reached out to me about generating correlated binary data. One user was not impressed by the copula algorithm that is already implemented. I’ve added an option to use an algorithm developed by Emrich and Piedmonte in 1991, and will be incorporating that option soon in the functions genCorGen and&amp;hellip;</description>
    </item>
    
    <item>
      <title>Generate animated tracking maps for hurricanes and typhoons</title>
      <link>/post/generate-animated-tracking-map/</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/generate-animated-tracking-map/</guid>
      <description>As of now, Sep. 11th, 2018, Hurricane Florence (Category 4) is approaching to eastern coastal areas of United States. There are many ways to track the path of hurricanes (or typhoons) to conduct further analysis, such as flash flooding, ocean surge and property damages. In this blog, the author tries to plot the tracking of hurricanes, using both stastic and animated maps, with the help of R and related packages. Specifically, this blog is a reproducible&amp;hellip;</description>
    </item>
    
    <item>
      <title>Bayesian Multilevel Model with Missing Data Complete Workflow (Part 2 of 3)</title>
      <link>/post/bayesian-multilevel-model-with/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-multilevel-model-with/</guid>
      <description>Overview: This is the second post in a three-part blog series I am putting together. If you have not read the first post in this series, you may want to go back and check it&amp;hellip;</description>
    </item>
    
    <item>
      <title>Visualizing Temperature Rise in Stuttgart, Germany over Time</title>
      <link>/post/visualizing-temperature-rise-i/</link>
      <pubDate>Sat, 01 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/visualizing-temperature-rise-i/</guid>
      <description>This is a quick use-case of gganimate to visualize the rise of average temperature in my home town, Stuttgart, Germany. Get&amp;hellip;</description>
    </item>
    
    <item>
      <title>How does Collinearity Influence Linear Regressions?</title>
      <link>/post/how-does-collinearity-influenc/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/how-does-collinearity-influenc/</guid>
      <description>This is a short simulation study trying to figure out the impact of collinearity on linear regressions. Overview: Packages Simulation Function Simulate and Save Data Visualizing the Influence of Collinearity Standard Errors T-Statistic and P-Values B-Coefficients Animation Standardized Packages Load the necessary&amp;hellip;</description>
    </item>
    
    <item>
      <title>Moving beyond pattern-based analysis</title>
      <link>/post/moving-beyond-patternbased-ana/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/moving-beyond-patternbased-ana/</guid>
      <description>This is the sixth and the last blog post in the series introducing GeoPAT 2 - a software for pattern-based spatial and temporal analysis . In the previous one we presented the pattern-based spatial segmentation - a method for creating regions of homogenous&amp;hellip;</description>
    </item>
    
    <item>
      <title>My first gganimate - exploring concepts from first year linear modelling!</title>
      <link>/post/my-first-gganimate-exploring-c/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/my-first-gganimate-exploring-c/</guid>
      <description>Have you ever had one of those moments whilst teaching where the content blows your mind? Today, whilst teaching MATH1005 at the University of Sydney, that exact thing happened to me. This weeks content was focused on teaching the students the introductions to linear modelling. A very strightforward topic, and one ususally understood well by first&amp;hellip;</description>
    </item>
    
    <item>
      <title>The power of stepped-wedge designs</title>
      <link>/post/the-power-of-steppedwedge-desi/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/the-power-of-steppedwedge-desi/</guid>
      <description>Just before heading out on vacation last month, I put up a post that purported to compare stepped-wedge study designs with more traditional cluster randomized trials. Either because I rushed or was just lazy, I didn’t exactly do what I set out to do. I did confirm that a multi-site randomized clinical trial can be more efficient than a cluster randomized trial when there is variability across&amp;hellip;</description>
    </item>
    
    <item>
      <title>OPP Interviews</title>
      <link>/post/opp-interviews/</link>
      <pubDate>Sun, 26 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/opp-interviews/</guid>
      <description>In this OPP series we will be interviewing scientists in Plant Pathology or related areas who has embraced open science and contributed knowledge and tools to advance the field. Our first OPP interview features Dr. Niklaus Grünwald ( @PhytophthoraLab on Twitter ), a plant pathologist with USDA ARS, Corvallis, OR, USA. Nik is well recognized by his research on population genetics and genomics of plant pathogens - mainly on oomycetes of major importance to global&amp;hellip;</description>
    </item>
    
    <item>
      <title>Analyzing Tweets of the ECPR General Conference 2018</title>
      <link>/post/analyzing-tweets-of-the-ecpr-g/</link>
      <pubDate>Sat, 25 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/analyzing-tweets-of-the-ecpr-g/</guid>
      <description>This is a short notebook outlining the code used to scrape tweets related to the ECPR Conference 2018 in Hamburg. Packages Load the necessary&amp;hellip;</description>
    </item>
    
    <item>
      <title>Most underrated advice for grad school</title>
      <link>/post/most-underrated-advice-for-gra/</link>
      <pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/most-underrated-advice-for-gra/</guid>
      <description>There is a ton of grad school advice out there (e.g. see Jennifer Doleac’s wonderful collection of Twitter threads) , but what are the most underrated pieces of advice? What is something you could tell an early grad student that they are less likely to have heard from someone else? This is my highly subjective list for OSU students. Invest in your workflow If you plan to work with any sort of data, invest heavily in your&amp;hellip;</description>
    </item>
    
    <item>
      <title>It&#39;s Alive! First Evidence that IBI VizEdit Works</title>
      <link>/post/its-alive-first-evidence-that-/</link>
      <pubDate>Thu, 16 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/its-alive-first-evidence-that-/</guid>
      <description>It is official. The program I have spent the better part of a year working on, the very centerpiece of my dissertation, works. Or at least, early indicators are in, and based on 22 cases, some of which required a great deal of manual editing, the program is returning estimates in line with expectations. Backing up, as I trip a little over my excitement, IBI VizEdit is an Rshiny application I created to help our lab process and edit heart rate&amp;hellip;</description>
    </item>
    
    <item>
      <title>Tools for getting started with your PhD</title>
      <link>/post/tools-for-getting-started-with/</link>
      <pubDate>Thu, 09 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tools-for-getting-started-with/</guid>
      <description>What this post is about Reference Manager Note-taking Keep track of the literature Follow blogs Join Twitter Give podcasts a try Familiarize yourself with preregistration and open science Learn R Freshen up your stats Closing remarks What this post is about The new academic semester is almost upon us, and that means lots of new grad&amp;hellip;</description>
    </item>
    
    <item>
      <title>Reading vintage magazines with `hocr`</title>
      <link>/post/reading-vintage-magazines-with/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/reading-vintage-magazines-with/</guid>
      <description>library(tidyverse) library(tesseract) library(pdftools) library(hocr) library(here) library(fs) library(hunspell) library(hrbrthemes) library(patchwork) Challenge This post is inspired by recent tweet by Paige Bailey about vintage computer magazines made available for free download on archive.org. A number of people picked up on the idea of checking out some of the old magazines from the time they can remeber starting with&amp;hellip;</description>
    </item>
    
    <item>
      <title>Randomize by, or within, cluster?</title>
      <link>/post/randomize-by-or-within-cluster/</link>
      <pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/randomize-by-or-within-cluster/</guid>
      <description>I am involved with a stepped-wedge designed study that is exploring whether we can improve care for patients with end-stage disease who show up in the emergency room. The plan is to train nurses and physicians in palliative care. (A while ago, I described what the stepped wedge design is.) Under this design, 33 sites around the country will receive the training at some point, which is no small task (and fortunately as the statistician, this is a part of the study I have little&amp;hellip;</description>
    </item>
    
    <item>
      <title>how should I get started with R?</title>
      <link>/post/how-should-i-get-started-with-/</link>
      <pubDate>Sun, 15 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/how-should-i-get-started-with-/</guid>
      <description>Here’s some evergreen advice from David Robinson: When you’ve written the same code 3 times, write a function When you’ve given the same in-person advice 3 times, write a blog post - David Robinson (@drob) November 9,&amp;hellip;</description>
    </item>
    
    <item>
      <title>Generate a reproducible map for county-level fertilizer estimation data in U.S.A. using R</title>
      <link>/post/generate-a-reproducible-map-fo/</link>
      <pubDate>Fri, 13 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/generate-a-reproducible-map-fo/</guid>
      <description>Introduction and motivation Nutrient input to agricultural watersheds is a very popular topic among researchers, engineers and stakeholders. Researchers in United State Geographic Services (USGS) spent a considerable amount of time and efforts to generate fertilizr estimation dataset from synthetic fertilizer and&amp;hellip;</description>
    </item>
    
    <item>
      <title>Talk</title>
      <link>/post/talk/</link>
      <pubDate>Fri, 13 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/talk/</guid>
      <description>References JJ Allaire, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng and Winston Chang (2018). rmarkdown: Dynamic Documents for R. R package version 1.10. https://CRAN.R-project.org/package=rmarkdown Ben Marwick (2018). rrtools: Creates a reproducible research compendium. R package version 0.1.0. https://github.com/benmarwick/rrtools RStudio Team (2016). RStudio: Integrated Development for R. RStudio, Inc., Boston,&amp;hellip;</description>
    </item>
    
    <item>
      <title>Interaction Plots with Continuous Moderators in R</title>
      <link>/post/interaction-plots-with-continu/</link>
      <pubDate>Wed, 11 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/interaction-plots-with-continu/</guid>
      <description>Long ago (the first half of my grad school life), I created a model for a manuscript I submitted. The paper was focused on adolescents’ appraisals of their relationships with their mothers, fathers, and best&amp;hellip;</description>
    </item>
    
    <item>
      <title>How the odds ratio confounds</title>
      <link>/post/how-the-odds-ratio-confounds/</link>
      <pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/how-the-odds-ratio-confounds/</guid>
      <description>The odds ratio always confounds: while it may be constant across different groups or clusters, the risk ratios or risk differences across those groups may vary quite substantially. This makes it really hard to interpret an&amp;hellip;</description>
    </item>
    
    <item>
      <title>Tuition Increases for Tidy Tuesday</title>
      <link>/post/tuition-increases-for-tidy-tue/</link>
      <pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tuition-increases-for-tidy-tue/</guid>
      <description>Here’s the code for my first Tidy Tuesday&amp;hellip;</description>
    </item>
    
    <item>
      <title>Linear Models</title>
      <link>/post/linear-models/</link>
      <pubDate>Fri, 06 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/linear-models/</guid>
      <description>Introduction library(tidyverse) library(matlib) library(knitr) library(RColorBrewer) The purpose of this document is to understand the parameter and residuals error estimates in a basic linear regression model when working with binary categorical variables. Recall the general model definition: [ \mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{e}] where (\mathbf{X}) is the design matrix and (\mathbf{\beta}) is a ((p+1))-vector of coefficients/parameters, including the intercept&amp;hellip;</description>
    </item>
    
    <item>
      <title>Wide data to long using the tidyverse (tidyr&#39;s gather function)</title>
      <link>/post/wide-data-to-long-using-the-ti/</link>
      <pubDate>Fri, 06 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/wide-data-to-long-using-the-ti/</guid>
      <description>A wide data storage format is an efficient and compact way to store information. And this organization perhaps it makes data easier to inspect. We have wide monitors our laptops and destops. However, for visualization and analysis you generally need to transform this data from the wide format to a “tidy”, long format. We look at the case where just one variable is stored in a&amp;hellip;</description>
    </item>
    
    <item>
      <title>Bayesian Multilevel Model with Missing Data</title>
      <link>/post/bayesian-multilevel-model-with/</link>
      <pubDate>Thu, 05 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-multilevel-model-with/</guid>
      <description>Overview: This is the first post in a three-part blog series I am putting together. The focus of this initial post is effective exploration of the reasons for missingness in a particular set of data. The second post in the series will focus on running and evaluating the imputation model itself after having identified the appropriate covariates that help account for&amp;hellip;</description>
    </item>
    
    <item>
      <title>Life (expectancy), animated</title>
      <link>/post/life-expectancy-animated/</link>
      <pubDate>Thu, 05 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/life-expectancy-animated/</guid>
      <description>Global socio-economic data is easily accessible nowadays. Just type the indicator of interest and the name of the country in your preferred search engine and you can find its value, sometimes also an additional plot or a&amp;hellip;</description>
    </item>
    
    <item>
      <title>Re-referencing factor levels to estimate standard errors when there is interaction turns out to be a really simple solution</title>
      <link>/post/rereferencing-factor-levels-to/</link>
      <pubDate>Tue, 26 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/rereferencing-factor-levels-to/</guid>
      <description>Maybe this should be filed under topics that are so obvious that it is not worth writing about. But, I hate to let a good simulation just sit on my computer. I was recently working on a paper investigating the relationship of emotion knowledge (EK) in very young kids with academic performance a year or two later. The idea is that kids who are more emotionally intelligent might be better prepared to&amp;hellip;</description>
    </item>
    
    <item>
      <title>Selection effects</title>
      <link>/post/selection-effects/</link>
      <pubDate>Mon, 25 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/selection-effects/</guid>
      <description>Saw this tweet by @Nolan_Mc , and thought, this is good fodder for a quick blog post – a true one-hour one-off. Run this Stata code, and then tell me the logic is sound set obs 20000 gen x =runiform() gen y = x + .1*rnormal() corr x y corr x y if x &amp;gt; .95 https://t.co/z5bE0iAH3I - Nolan McCarty (@Nolan_Mc) June 17,&amp;hellip;</description>
    </item>
    
    <item>
      <title>Naming Things</title>
      <link>/post/naming-things/</link>
      <pubDate>Wed, 20 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/naming-things/</guid>
      <description>There are only two hard things in Computer Science: cache invalidation and naming thing - Phil Karlton ( seen in r-packages ) I&amp;rsquo;m not really sure what cache invalidation is, but I certainly agree that naming things is pretty damn hard. I thought it would be interesting to share some thoughts and discussion that I&amp;rsquo;ve been having about naming packages. There was some recent discussion on tidy names on Twitter, and I felt like this was a good time to discuss this&amp;hellip;</description>
    </item>
    
    <item>
      <title>Federalist Papers</title>
      <link>/post/federalist-papers/</link>
      <pubDate>Sat, 16 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/federalist-papers/</guid>
      <description>Every couple of weeks I like to explore data that’s brand new to me. I anticipate a one-hour, one-off project. Usually this turns out to be a beautiful lie, and the projects chew up much more time. Still, this enticing time-line is pulling me into new projects from time to time. Earlier this week, I heard about the dispute of authorship of some of the Federalist papers. &amp;ldquo;Hamilton wrote the other 51!</description>
    </item>
    
    <item>
      <title>Re-introduction to gghighlight</title>
      <link>/post/reintroduction-to-gghighlight/</link>
      <pubDate>Sat, 16 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/reintroduction-to-gghighlight/</guid>
      <description>Half a year ago, I&amp;rsquo;ve introduced gghighlight package. I didn&amp;rsquo;t expect so much R people get interested in my package. Thanks for your attention! But, please forget about that gghighlight; gghighlight has become far more powerful and simple! So, let me re-introduce about gghighlight. (Note that this version of gghighlight is not yet on CRAN at the time of this&amp;hellip;</description>
    </item>
    
    <item>
      <title>Late anniversary edition redux</title>
      <link>/post/late-anniversary-edition-redux/</link>
      <pubDate>Wed, 13 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/late-anniversary-edition-redux/</guid>
      <description>This afternoon, I was looking over some simulations I plan to use in an upcoming lecture on multilevel models. I created these examples a while ago, before I started this&amp;hellip;</description>
    </item>
    
    <item>
      <title>Eat near the Big Ben?  That will cost you...</title>
      <link>/post/eat-near-the-big-ben-that-will/</link>
      <pubDate>Tue, 12 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/eat-near-the-big-ben-that-will/</guid>
      <description>What’s the cost of eating near an iconic location? We all might suspect eating in a fun location is pricier. I used the menu and restaurant location data that #MakeoverMonday “assigned” one week last December to demonstrate this among London Wetherspoon Pubs . #MakeoverMonday is a fun data visualization initiative; most participants use Tableau as their preferred visualization tool. But I’ve used R and ggplot() and the organizers and participants have been very&amp;hellip;</description>
    </item>
    
    <item>
      <title>tiler 0.2.0 CRAN release</title>
      <link>/post/tiler-020-cran-release/</link>
      <pubDate>Mon, 11 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tiler-020-cran-release/</guid>
      <description>The tiler package provides a map tile-generator function for creating map tile sets for use with packages such as leaflet . In addition to generating map tiles based on a common raster layer source, it also handles the non-geographic edge case, producing map tiles from arbitrary images. These map tiles, which have a “simple CRS”, a non-geographic simple Cartesian coordinate reference system, can also be used with leaflet when applying the simple CRS&amp;hellip;</description>
    </item>
    
    <item>
      <title>Plot geom_sf() On OpenStreetMap Tiles</title>
      <link>/post/plot-geomsf-on-openstreetmap-t/</link>
      <pubDate>Sat, 09 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/plot-geomsf-on-openstreetmap-t/</guid>
      <description>mapview is a very nice package to explore an sf object. It can overlay sf object on the map&amp;hellip;</description>
    </item>
    
    <item>
      <title>Article Round Up June 2018</title>
      <link>/post/article-round-up-june-2018/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/article-round-up-june-2018/</guid>
      <description>This is my second article round up. The first is over at my old blog here . In this post, I’ll briefly cover two Atlantic articles related to economic inequality and the differing economic expectations of democrats and&amp;hellip;</description>
    </item>
    
    <item>
      <title>Fair is foul, and foul is fair</title>
      <link>/post/fair-is-foul-and-foul-is-fair/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/fair-is-foul-and-foul-is-fair/</guid>
      <description>Sentiment analysis can be used for many purposes and applied to all kinds of texts. In this exploratory analysis, we’ll use a tidytext approach to examine the use of sentiment words in the tragedies written by William Shakespeare. I’ve previously used Python for scraping and mining texts. However, I recently stumbled upon the tidytext R package by Julia Silge and David Robinson as well as their excellent book and ressource on combining tidytext with other tidy tools in&amp;hellip;</description>
    </item>
    
    <item>
      <title>Power Analyses for an Unconditional Growth Model using {lmer}</title>
      <link>/post/power-analyses-for-an-uncondit/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/power-analyses-for-an-uncondit/</guid>
      <description>Recently, I was asked to knock together a quick power analysis for a linear growth model with approximately 120&amp;hellip;</description>
    </item>
    
    <item>
      <title>A note on factors in regression (in R)</title>
      <link>/post/a-note-on-factors-in-regressio/</link>
      <pubDate>Sat, 02 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-note-on-factors-in-regressio/</guid>
      <description>Factors terrify me. I can avoid dealing with them most of the time, but they’re immensely useful in a regression when you have a categorical variable with many levels (e.g. “Very Bad”, “Bad”, “Good”, “Very Good”). A common example in economics is when you have a difference-in-differences design and you want to estimate how the treatment effect changes over time, before and after the treatment period. Here’s a toy&amp;hellip;</description>
    </item>
    
    <item>
      <title>A little function to help generate ICCs in simple clustered data</title>
      <link>/post/a-little-function-to-help-gene/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-little-function-to-help-gene/</guid>
      <description>In health services research, experiments are often conducted at the provider or site level rather than the patient level. However, we might still be interested in the outcome at the patient level. For example, we could be interested in understanding the effect of a training program for physicians on their patients. It would be very difficult to randomize patients to be exposed or not to the training if a group of patients all see the same&amp;hellip;</description>
    </item>
    
    <item>
      <title>Gaussian Process Imputation/Forecast Models</title>
      <link>/post/gaussian-process-imputationfor/</link>
      <pubDate>Mon, 21 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/gaussian-process-imputationfor/</guid>
      <description>A well-established set of problems emerges when attempting to analyze non-stationary univariate time series (i.e., the signal’s mean and/or variance changes over time). A common approach is to impose some stationarity on the data so that certain modeling techniques can provide allow a research to make some predictions (e.g., ARIMA&amp;hellip;</description>
    </item>
    
    <item>
      <title>What is going on in OPP? a quick summary of the first five months</title>
      <link>/post/what-is-going-on-in-opp-a-quic/</link>
      <pubDate>Sun, 20 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/what-is-going-on-in-opp-a-quic/</guid>
      <description>We founded OPP in early January this year without too much ambition and zero&amp;hellip;</description>
    </item>
    
    <item>
      <title>Modeling the error variance to account for heteroskedasticity</title>
      <link>/post/modeling-the-error-variance-to/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/modeling-the-error-variance-to/</guid>
      <description>One of the assumptions that comes with applying OLS estimation for regression models in the social sciences is homoskedasticity, I prefer constant error variance (it also goes by spherical disturbances). It implies that there is no systematic pattern to the error variance, meaning the model is equally poor at all levels of prediction. This assumption is important for OLS to be the best linear unbiased predictor&amp;hellip;</description>
    </item>
    
    <item>
      <title>Simulating data from regression models</title>
      <link>/post/simulating-data-from-regressio/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/simulating-data-from-regressio/</guid>
      <description>My preferred approach to validating regression models is to simulate data from them, and see if the simulated data capture relevant features of the original data. A basic feature of interest would be the mean. I like this approach because it is extendable to the family of generalized linear models (logistic, Poisson, gamma, &amp;hellip;) and other regression models, say t-regression. It&amp;rsquo;s something Gelman and Hill cover in their regression&amp;hellip;</description>
    </item>
    
    <item>
      <title>How efficient are multifactorial experiments?</title>
      <link>/post/how-efficient-are-multifactori/</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/how-efficient-are-multifactori/</guid>
      <description>I recently described why we might want to conduct a multi-factorial experiment, and I alluded to the fact that this approach can be quite efficient. It is efficient in the sense that it is possible to test simultaneously the impact of multiple interventions using an overall sample size that would be required to test a single intervention in a more traditional&amp;hellip;</description>
    </item>
    
    <item>
      <title>Testing multiple interventions in a single experiment</title>
      <link>/post/testing-multiple-interventions/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/testing-multiple-interventions/</guid>
      <description>A reader recently inquired about functions in simstudy that could generate data for a balanced multi-factorial design. I had to report that nothing really exists. A few weeks later, a colleague of mine asked if I could help estimate the appropriate sample size for a study that plans to use a multi-factorial design to choose among a set of interventions to improve rates of smoking&amp;hellip;</description>
    </item>
    
    <item>
      <title>tabr package for guitar tablature now on CRAN</title>
      <link>/post/tabr-package-for-guitar-tablat/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tabr-package-for-guitar-tablat/</guid>
      <description>The tabr package for creating guitar tablature (&amp;ldquo;tabs&amp;rdquo;) from R code is now available on CRAN. tabr provides programmatic music notation and a wrapper around LilyPond for creating quality guitar tablature. tabr offers functions for describing and organizing musical structures and wraps around the LilyPond backend. LilyPond is an open source music engraving program for generating high quality sheet music based on markup&amp;hellip;</description>
    </item>
    
    <item>
      <title>Exploring Tuberculosis Monitoring Indicators in England; Using Dimension Reduction and Clustering</title>
      <link>/post/exploring-tuberculosis-monitor/</link>
      <pubDate>Wed, 11 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/exploring-tuberculosis-monitor/</guid>
      <description>Introduction I recently attended the Public Health Research and Science Conference , run by Public Health England (PHE), at the University of Warwick. I was mainly there to present some work that I have been doing (along with my co-authors) estimating the direct effects of the 2005 change in BCG vaccination policy on Tuberculosis (TB) incidence rates ( slides ) but it was also a great opportunity to see what research is being done within, and partnered with,&amp;hellip;</description>
    </item>
    
    <item>
      <title>TidyTuesday - A weekly social data project in R</title>
      <link>/post/tidytuesday-a-weekly-social-da/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday-a-weekly-social-da/</guid>
      <description>Over the past month or so, the r4ds online learning community founded by Jesse Maegan has been developing projects intended to help connect mentors and&amp;hellip;</description>
    </item>
    
    <item>
      <title>Kaggle panel recap</title>
      <link>/post/kaggle-panel-recap/</link>
      <pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/kaggle-panel-recap/</guid>
      <description>Introduction This past March I had the distinct pleasure of participating in a panel about making the career transition to data science as part of Kaggle’s CareerCon&amp;hellip;</description>
    </item>
    
    <item>
      <title>A new RStudio addin to facilitate inserting tables in Rmarkdown documents</title>
      <link>/post/a-new-rstudio-addin-to-facilit/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-new-rstudio-addin-to-facilit/</guid>
      <description>In the last months, I started increasingly using Rmd documents for preparing scientific reports, blog posts, etcetera. While I really like the flexibility offered by the system, one thing that I thought could be improved is the support for easily inserting&amp;hellip;</description>
    </item>
    
    <item>
      <title>Note to Self</title>
      <link>/post/note-to-self/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/note-to-self/</guid>
      <description>This is the first post in a series where I write to myself regarding the various data science spells I’m learning. Today’s spell: dplyr’s filter function. For some reason, upon learning how to filter data with the dplyr package, I thought that function was designed to only remove or discard data, specifically columns. That is not the case and I’m writing this blog post to try and correct this automatic thinking in my&amp;hellip;</description>
    </item>
    
    <item>
      <title>YMMV: non-profit data science</title>
      <link>/post/ymmv-nonprofit-data-science/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ymmv-nonprofit-data-science/</guid>
      <description>(YMMV = your mileage may vary) Introduction Feeling inspired by some recent data science collaborations, on Friday I released the following tweet into the wild: want to build data science experience? reach out to a local non-profit you&amp;#39;re interested in, and ask them if you can volunteer with data collection, cleaning, and basic analysis and reporting. you get experience, the NPO gets a product they desperately need, and everyone wins. - Jesse Mostipak (@kierisi) March 30,&amp;hellip;</description>
    </item>
    
    <item>
      <title>Great Circles with R</title>
      <link>/post/great-circles-with-r/</link>
      <pubDate>Wed, 28 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/great-circles-with-r/</guid>
      <description>In 1569 the Flemish cartographer and mathematician Gerardus Mercator published a new world map under the title &amp;ldquo;New and more complete representation of the terrestrial globe properly adapted for use in navigation.&amp;rdquo; The title of the map points to Mercator&amp;rsquo;s main claim for its usefulness, which he expounded upon in the map&amp;rsquo;s legends. Mercator presented his map as not only an accurate representation of the known world, but also as a particularly useful map for the purposes of&amp;hellip;</description>
    </item>
    
    <item>
      <title>Exploring the underlying theory of the chi-square test through simulation - part 2</title>
      <link>/post/exploring-the-underlying-theor/</link>
      <pubDate>Sun, 25 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/exploring-the-underlying-theor/</guid>
      <description>In the last post , I tried to provide a little insight into the chi-square test. In particular, I used simulation to demonstrate the relationship between the Poisson distribution of counts and the chi-squared distribution. The key point in that post was the role conditioning plays in that relationship by reducing variance. To motivate some of the key issues, I talked a bit about recycling. I asked you to imagine a set of bins placed in different locations to collect glass&amp;hellip;</description>
    </item>
    
    <item>
      <title>Introducing tabr</title>
      <link>/post/introducing-tabr/</link>
      <pubDate>Mon, 19 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/introducing-tabr/</guid>
      <description>This post introduces a new R package I am working on called tabr for creating guitar tablature (&amp;ldquo;tabs&amp;rdquo;) from R code. The tabr package provides programmatic music notation and a wrapper around LilyPond for creating quality guitar tablature. tabr offers functions for describing and organizing musical structures and wraps around the LilyPond backend. LilyPond is an open source music engraving program for generating high quality sheet music based on markup&amp;hellip;</description>
    </item>
    
    <item>
      <title>Functional programming in R with Purrr</title>
      <link>/post/functional-programming-in-r-wi/</link>
      <pubDate>Sun, 18 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/functional-programming-in-r-wi/</guid>
      <description>When you first started in R you likely were writing simple code to generate one&amp;hellip;</description>
    </item>
    
    <item>
      <title>A gentle guide to Tidy statistics in R</title>
      <link>/post/a-gentle-guide-to-tidy-statist/</link>
      <pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-gentle-guide-to-tidy-statist/</guid>
      <description>While data analysis in R can seem intimidating, we will explore how to use it effectively and clearly! Introduction After a great discussion started by Jesse Maegan ( @kiersi ) on Twitter, I decided to post a workthrough of some (fake) experimental treatment data. These data correspond to a new (fake) research drug called AD-x37, a theoretical drug that has been shown to have beneficial outcomes on cognitive decline in mouse models of Alzheimer’s&amp;hellip;</description>
    </item>
    
    <item>
      <title>Contra JFK, use R because it is easy, not because it is hard</title>
      <link>/post/contra-jfk-use-r-because-it-is/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/contra-jfk-use-r-because-it-is/</guid>
      <description>p.caption { font-size: 0.8em; } “Coding is easy” There’s an attitude that I believe is reasonably common among applied economists, that coding is the easy part of what we do and where we add the least&amp;hellip;</description>
    </item>
    
    <item>
      <title>EPL Week 30</title>
      <link>/post/epl-week-30/</link>
      <pubDate>Wed, 14 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/epl-week-30/</guid>
      <description>For the remainder of the season, I will be travelling with a back up laptop so please excuse any shortfall in posts and site updates Match of the DayRashford schools Alex-Arnold Palace joined WBA with a league-leading ninth one-goal defeat. Every team has suffered at least one such occurrence this season. Surprisingly Huddersfield have lost by a single goal just once in their 15 defeatsManager changesSeveral teams have changed manager’s this season in an attempt to acquire a ‘New manger&amp;hellip;</description>
    </item>
    
    <item>
      <title>Making maps of the USA with R</title>
      <link>/post/making-maps-of-the-usa-with-r/</link>
      <pubDate>Tue, 13 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/making-maps-of-the-usa-with-r/</guid>
      <description>Introduction Maps of United States often focus only on the contiguous 48 states. In many maps Alaska and Hawaii are simply not shown or are displayed at different geographic scales than the main map. This article shows how to create inset maps of the USA, building on a chapter in the in-development book Geocomputation with R that shows all its states and ensures relative sizes are preserved. It requires up-to-date versions of the following packages to be loaded:&amp;hellip;</description>
    </item>
    
    <item>
      <title>Another reason to be careful about what you control for</title>
      <link>/post/another-reason-to-be-careful-a/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/another-reason-to-be-careful-a/</guid>
      <description>Modeling data without any underlying causal theory can sometimes lead you down the wrong path, particularly if you are interested in understanding the way things work rather than making predictions. A while back, I described what can go wrong when you control for a mediator when you are interested in an exposure and an&amp;hellip;</description>
    </item>
    
    <item>
      <title>Interpretable Machine Learning with iml and mlr</title>
      <link>/post/interpretable-machine-learning/</link>
      <pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/interpretable-machine-learning/</guid>
      <description>Machine learning models repeatedly outperform interpretable, parametric models like the linear regression model. The gains in performance have a price: The models operate as black boxes which are not interpretable. Fortunately, there are many methods that can make machine learning models&amp;hellip;</description>
    </item>
    
    <item>
      <title>Training Courses for mlr</title>
      <link>/post/training-courses-for-mlr/</link>
      <pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/training-courses-for-mlr/</guid>
      <description>The mlr: Machine Learning in R package provides a generic, object-oriented and extensible framework for classification, regression, survival analysis and clustering for the statistical programming language R. The package targets practitioners who want to quickly apply machine learning algorithms, as well as researchers who want to implement, benchmark, and compare their new methods in a structured environment. We are happy to announce that we now offer training courses specialized on&amp;hellip;</description>
    </item>
    
    <item>
      <title>Fundamentos de Inferencia Estadística</title>
      <link>/post/fundamentos-de-inferencia-esta/</link>
      <pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/fundamentos-de-inferencia-esta/</guid>
      <description>Introducción A pesar de que en la formación en psicología se nos ofrecen varios cursos sobre estadística descriptiva e inferencial, difícilmente los estudiantes comprenden a qué se refiere exactamente el tema. Es común, relacionar la inferencia con la aplicación de pruebas estadísticas&amp;hellip;</description>
    </item>
    
    <item>
      <title>Brief introduction of storm hysteresis effects in solute concentration-stream discharge (C-Q) relationship</title>
      <link>/post/brief-introduction-of-storm-hy/</link>
      <pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/brief-introduction-of-storm-hy/</guid>
      <description>What are storm hysteresis effects? Generally, in order to investigate the dynamics of stream discharge and solute concentrations (C-Q relationship) in a watershed, researchers and environmental engineers usually set up monitoring stations in the watershed&amp;hellip;</description>
    </item>
    
    <item>
      <title>Econothemes</title>
      <link>/post/econothemes/</link>
      <pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/econothemes/</guid>
      <description>econothemes can be installed with the devtools package in R by executing devtools::install_github(&amp;ldquo;weiyangtham/econothemes&amp;rdquo;) . More details available on the Github&amp;hellip;</description>
    </item>
    
    <item>
      <title>“I have to randomize by cluster. Is it OK if I only have 6 sites?&#39;</title>
      <link>/post/i-have-to-randomize-by-cluster/</link>
      <pubDate>Wed, 21 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/i-have-to-randomize-by-cluster/</guid>
      <description>The answer is probably no, because there is a not-so-low chance (perhaps considerably higher than 5%) you will draw the wrong conclusions from the study. I have heard variations on this question not so infrequently, so I thought it would be useful (of course) to do a few quick simulations to see what happens when we try to conduct a study under these&amp;hellip;</description>
    </item>
    
    <item>
      <title>Scraping NIH PIs with rvest</title>
      <link>/post/scraping-nih-pis-with-rvest/</link>
      <pubDate>Thu, 15 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/scraping-nih-pis-with-rvest/</guid>
      <description>1 The Big Picture 2 Scraping Investigator Names 2.1 Clean names 3 OPM data 3.1 Linking PI names to OPM payroll data 4 What can we learn about the intramural program? 4.1 Aging of the scientifc workforce 4.2 How much do scientists earn? 4.3 Gender representation in the intramural program Background: I was doing some exploratory work for a potential project looking at intramural investigators at the&amp;hellip;</description>
    </item>
    
    <item>
      <title>Books I Reference</title>
      <link>/post/books-i-reference/</link>
      <pubDate>Tue, 13 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/books-i-reference/</guid>
      <description>The full list of the books in my shelf is on my Goodreads account 1. The ones I refer to the most are listed here: Deep Learning Deep Learning with R Francois Chollet Handbook Of Neural Computing Applications Alianna J Maren Deep Learning Ian Goodfellow LSTM with Python Jason Brownlee GLM Generalized Additive Models: An Introduction with R, Second Edition Simon Wood Applied Regression Modeling Iain Pardoe Generalized Linear Models John&amp;hellip;</description>
    </item>
    
    <item>
      <title>Using binary regression software to model ordinal data as a multivariate GLM</title>
      <link>/post/using-binary-regression-softwa/</link>
      <pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/using-binary-regression-softwa/</guid>
      <description>I have read that the most common model for analyzing ordinal data is the cumulative link logistic model, coupled with the proportional odds assumption. Essentially, you treat the outcome as if it were the categorical manifestation of a continuous latent variable. The predictor variables of this outcome influence it in one way only, so you get a single regression coefficient for each&amp;hellip;</description>
    </item>
    
    <item>
      <title>Introduction to GIS with R</title>
      <link>/post/introduction-to-gis-with-r/</link>
      <pubDate>Thu, 08 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/introduction-to-gis-with-r/</guid>
      <description>The geographic visualization of data makes up one of the major branches of the Digital Humanities toolkit. There are a plethora of tools that can visualize geographic information from full-scale GIS applications such as ArcGIS and QGIS to web-based tools like Google maps to any number of programing languages. There are advantages and disadvantages to these different types of&amp;hellip;</description>
    </item>
    
    <item>
      <title>Have you ever asked yourself, &#39;how should I approach the classic pre-post analysis?&#39;</title>
      <link>/post/have-you-ever-asked-yourself-h/</link>
      <pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/have-you-ever-asked-yourself-h/</guid>
      <description>Well, maybe you haven’t, but this seems to come up all the time. An investigator wants to assess the effect of an intervention on a outcome. Study participants are randomized either to receive the intervention (could be a new drug, new protocol, behavioral intervention, whatever) or treatment as usual. For each participant, the outcome measure is recorded at baseline - this is the pre in pre/post&amp;hellip;</description>
    </item>
    
    <item>
      <title>Trump VS Clinton Interpretable Text Classifier</title>
      <link>/post/trump-vs-clinton-interpretable/</link>
      <pubDate>Fri, 26 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/trump-vs-clinton-interpretable/</guid>
      <description>I&amp;rsquo;ve been writing/talking a lot about LIME recently: in this blog / at H20 meetup , or at coming AI Congress and I&amp;rsquo;m still sooo impressed by this tool for interpreting any, even black-box, algorithm! The part I love most is that LIME can be applied to both image and text data, that was well showcased in husky VS wolf (image) and Christian VS atheist (text) examples in the original&amp;hellip;</description>
    </item>
    
    <item>
      <title>Geocomputation with R - the intermission</title>
      <link>/post/geocomputation-with-r-the-inte/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/geocomputation-with-r-the-inte/</guid>
      <description>Hello everybody! A lot of things have changed since the last blogpost about Geocomputation with R . In this post I’ll give an update of our progress and our plans for the next chapters. Third author Probably the most important change is having a third author - Jannes Muenchow . He is a GIScientist based at the University of Jena with a keen interest in spatial and geostatistical modeling, algorithm automation and&amp;hellip;</description>
    </item>
    
    <item>
      <title>Linear Models</title>
      <link>/post/linear-models/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/linear-models/</guid>
      <description>Preamble The purpose of this post is to elucidate some of the concepts associated with statistical linear models. Let’s start by loading some libraries. library(ggplot2) library(datasets) Background Theory The basic idea is as follows: Given two variables, (x) and (y), for which we’ve measured a set of data points ({x_i, y_i}) with (i = 1, &amp;hellip;, n), we want to estimate a function, (f(x)), such that [y_i = f(x_i) +&amp;hellip;</description>
    </item>
    
    <item>
      <title>Elementos de Estadística Inferencial en R</title>
      <link>/post/elementos-de-estad%C3%ADstica-infer/</link>
      <pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/elementos-de-estad%C3%ADstica-infer/</guid>
      <description>Descripción: Curso de nivel básico para el análisis estadístico en Psicología mediante el lenguaje de programación&amp;hellip;</description>
    </item>
    
    <item>
      <title>Importance sampling adds an interesting twist to Monte Carlo simulation</title>
      <link>/post/importance-sampling-adds-an-in/</link>
      <pubDate>Thu, 18 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/importance-sampling-adds-an-in/</guid>
      <description>I’m contemplating the idea of teaching a course on simulation next fall, so I have been exploring various topics that I might include. (If anyone has great ideas either because you have taught such a course or taken one, definitely drop me a note.) Monte Carlo (MC) simulation is an obvious one. I like the idea of talking about importance sampling , because it sheds light on the idea that not all MC simulations are created&amp;hellip;</description>
    </item>
    
    <item>
      <title>Curated Collection of Teaching Materials</title>
      <link>/post/curated-collection-of-teaching/</link>
      <pubDate>Wed, 17 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/curated-collection-of-teaching/</guid>
      <description>Applied Linear Models (STAT3012 at USYD) So I&amp;#39;m trying something new for teaching&amp;hellip; Totally redesigned the slides, added a &amp;ldquo;Fun fact&amp;rdquo; to pay a homage to Ronald Fisher, and getting down with foundational concepts of experimental design (as per Bailey 2008). Maybe I should hold a short trivia at the end? 🤔 #statsed pic.twitter.com/bxlFUq8Db0 - Emi Tanaka 🌾 (@statsgen) May 10,&amp;hellip;</description>
    </item>
    
    <item>
      <title>Customer Churn - EDA</title>
      <link>/post/customer-churn-eda/</link>
      <pubDate>Tue, 16 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/customer-churn-eda/</guid>
      <description>Executive Summary Tenure is the biggest driver of churn, where ~42% of customer churn occurs during the first 6 months and 71.1% occurs within the first 12 months. Interestingly, 99.5% of the customer churn in the first 6 months and 98.9% in the first 12 months are customers on a month-to-month&amp;hellip;</description>
    </item>
    
    <item>
      <title>Exploring Global Trends in Tuberculosis Incidence Rates - with GetTBinR</title>
      <link>/post/exploring-global-trends-in-tub/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/exploring-global-trends-in-tub/</guid>
      <description>In November I attended Epidemics , which is a conference focused on modelling infectious diseases. There was a lot of great work and perhaps most excitingly a lot of work being offered as R packages. I’ve recently begun wrapping all my analytical work in R packages, as it makes producing reproducible research a breeze! Unfortunately all of this work is still making it’s way towards publication and for a variety of reasons can’t be shared until it has passed this&amp;hellip;</description>
    </item>
    
    <item>
      <title>Simulating a cost-effectiveness analysis to highlight new functions for generating correlated data</title>
      <link>/post/simulating-a-costeffectiveness/</link>
      <pubDate>Mon, 08 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/simulating-a-costeffectiveness/</guid>
      <description>My dissertation work (which I only recently completed - in 2012 - even though I am not exactly young, a whole story on its own) focused on inverse probability weighting methods to estimate a causal cost-effectiveness model. I don’t really do any cost-effectiveness analysis (CEA) anymore, but it came up very recently when some folks in the Netherlands contacted me about using simstudy to generate correlated (and clustered) data to compare different approaches to estimating&amp;hellip;</description>
    </item>
    
    <item>
      <title>Confounding and collinearity</title>
      <link>/post/confounding-and-collinearity/</link>
      <pubDate>Thu, 04 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/confounding-and-collinearity/</guid>
      <description>Introduction In this article, I will discuss about the bias introduced in estimation of coefficient of a given explanatory variable due to the presence of confounding&amp;hellip;</description>
    </item>
    
    <item>
      <title>Using glmer() to perform Rasch analysis</title>
      <link>/post/using-glmer-to-perform-rasch-a/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/using-glmer-to-perform-rasch-a/</guid>
      <description>I&amp;rsquo;ve been interested in the relationship between ordinal regression and item response theory (IRT) for a few months now. There are several helpful papers on the topic, here are some randomly picked ones 1 2 3 4 5, and a book.6 In this post, I focus on Rasch&amp;hellip;</description>
    </item>
    
    <item>
      <title>Introducing rhymer</title>
      <link>/post/introducing-rhymer/</link>
      <pubDate>Sat, 23 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/introducing-rhymer/</guid>
      <description>I’m proud to announce that I have published my first R package, rhymer ! Version 1.0 is now available on CRAN . You can view the source code on github , and see the website for the package here . The goal of rhymer is to provide an R interface to the Datamuse API in order to find rhyming words. The core function of rhymer is get_rhyme() , which can be used in a variety of&amp;hellip;</description>
    </item>
    
    <item>
      <title>Post-selection inference on Friends titles in R</title>
      <link>/post/postselection-inference-on-fri/</link>
      <pubDate>Fri, 22 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/postselection-inference-on-fri/</guid>
      <description>Goal I want to be a Friends scriptwriter. Can I pick a title that makes an episode an automatic classic? If I just include a character’s name in the title, does it make it automatically popular? I assume I should just write “The One Where Rachel is Rachel”. But let’s&amp;hellip;</description>
    </item>
    
    <item>
      <title>When there&#39;s a fork in the road, take it. Or, taking a look at marginal structural models.</title>
      <link>/post/when-theres-a-fork-in-the-road/</link>
      <pubDate>Mon, 11 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/when-theres-a-fork-in-the-road/</guid>
      <description>I am going to cut right to the chase, since this is the third of three posts related to confounding and weighting, and it’s kind of a long one. (If you want to catch up, the first two are here and here .) My aim with these three posts is to provide a basic explanation of the marginal structural model (MSM) and how we should interpret the&amp;hellip;</description>
    </item>
    
    <item>
      <title>When you use inverse probability weighting for estimation, what are the weights actually doing?</title>
      <link>/post/when-you-use-inverse-probabili/</link>
      <pubDate>Mon, 04 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/when-you-use-inverse-probabili/</guid>
      <description>Towards the end of Part 1 of this short series on confounding, IPW, and (hopefully) marginal structural models, I talked a little bit about the fact that inverse probability weighting (IPW) can provide unbiased estimates of marginal causal effects in the context of confounding just as more traditional regression models like OLS can. I used an example based on a normally distributed&amp;hellip;</description>
    </item>
    
    <item>
      <title>Visualizing how confounding biases estimates of population-wide (or marginal) average causal effects</title>
      <link>/post/visualizing-how-confounding-bi/</link>
      <pubDate>Thu, 16 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/visualizing-how-confounding-bi/</guid>
      <description>When we are trying to assess the effect of an exposure or intervention on an outcome, confounding is an ever-present threat to our ability to draw the proper&amp;hellip;</description>
    </item>
    
    <item>
      <title>A simstudy update provides an excuse to generate and display Likert-type data</title>
      <link>/post/a-simstudy-update-provides-an-/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/a-simstudy-update-provides-an-/</guid>
      <description>I just updated simstudy to version 0.1.7. It is available on CRAN. To mark the occasion, I wanted to highlight a new function, genOrdCat , which puts into practice some code that I presented a little while back as part of a discussion of ordinal logistic regression . The new function was motivated by a reader/researcher who came across my blog while wrestling with a simulation&amp;hellip;</description>
    </item>
    
    <item>
      <title>Automated and Unmysterious Machine Learning in Cancer Detection</title>
      <link>/post/automated-and-unmysterious-mac/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/automated-and-unmysterious-mac/</guid>
      <description>I get bored from doing two things: i) spot-checking + optimising parameters of my predictive models and ii) reading about how &amp;lsquo;black box&amp;rsquo; machine learning (particularly deep learning) models are and how little we can do to better understand how they learn (or not learn, for example when they take a panda bear for a&amp;hellip;</description>
    </item>
    
    <item>
      <title>Tidyverse Case Study</title>
      <link>/post/tidyverse-case-study/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/tidyverse-case-study/</guid>
      <description>Data packages are something that have been on my mind a bit lately , having recently worked on the ozroaddeaths data access package at the rOpenSci&amp;hellip;</description>
    </item>
    
    <item>
      <title>Thinking about different ways to analyze sub-groups in an RCT</title>
      <link>/post/thinking-about-different-ways-/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/thinking-about-different-ways-/</guid>
      <description>Here’s the scenario: we have an intervention that we think will improve outcomes for a particular&amp;hellip;</description>
    </item>
    
    <item>
      <title>Misspecification and fit indices in covariance-based SEM</title>
      <link>/post/misspecification-and-fit-indic/</link>
      <pubDate>Sat, 28 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/misspecification-and-fit-indic/</guid>
      <description>TLDR: If you have good measurement quality, conventional benchmarks for fit indices may lead to bad decisions. Additionally, global fit indices are not informative for investigating misspecification. I am working with one of my professors, Dr. Jessica Logan, on a checklist for the developmental progress of young children. We intend to take this down the IRT route (or ordinal logistic regression), but currently, this is all part of a factor analysis course&amp;hellip;</description>
    </item>
    
    <item>
      <title>Publish R Markdown to Medium via An RStudio Addin</title>
      <link>/post/publish-r-markdown-to-medium-v/</link>
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/publish-r-markdown-to-medium-v/</guid>
      <description>I created an experimental package to work with Medium API . mediumr: https://github.com/yutannihilation/mediumr/ mediumr allows you to knit and post R Markdown to Medium. Installation You can install mediumr from github&amp;hellip;</description>
    </item>
    
    <item>
      <title>Extending slackr</title>
      <link>/post/extending-slackr/</link>
      <pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/extending-slackr/</guid>
      <description>The slackr package, by Bob Rudis , contains functions that make it possible to interact with the Slack messaging platform. The great part of Slack is that it has mobile devices application&amp;hellip;</description>
    </item>
    
    <item>
      <title>How Not To Knit All Rmd Files With Blogdown</title>
      <link>/post/how-not-to-knit-all-rmd-files-/</link>
      <pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/how-not-to-knit-all-rmd-files-/</guid>
      <description>Blogdown is a cool package. But, if I complain about one thing, it will be the default behaviour of build_site() , which every blogdownners should execute everytime they wants to publish a new article. As stated in the documentation, build_site() will Compile all Rmd files and build the site through Hugo. ( ?build_site ) Compiling all Rmd files is &amp;ldquo;safe&amp;rdquo; in the sense that we can notice if some Rmd becomes impossible to compile due to some breaking changes of some&amp;hellip;</description>
    </item>
    
    <item>
      <title>Introduction to Network Analysis with R</title>
      <link>/post/introduction-to-network-analys/</link>
      <pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/introduction-to-network-analys/</guid>
      <description>Over a wide range of fields network analysis has become an increasingly popular tool for scholars to deal with the complexity of the interrelationships between actors of all sorts. The promise of network analysis is the placement of significance on the relationships between actors, rather than seeing actors as isolated&amp;hellip;</description>
    </item>
    
    <item>
      <title>apputils 0.5.0 released</title>
      <link>/post/apputils-050-released/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/apputils-050-released/</guid>
      <description>Version 0.5.0 of the apputils R package has been released on&amp;hellip;</description>
    </item>
    
    <item>
      <title>Who knew likelihood functions could be so pretty?</title>
      <link>/post/who-knew-likelihood-functions-/</link>
      <pubDate>Mon, 23 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/who-knew-likelihood-functions-/</guid>
      <description>I just released a new iteration of simstudy (version 0.1.6), which fixes a bug or two and adds several spline related routines (available on CRAN ). The previous post focused on using spline curves to generate data, so I won’t repeat myself here. And, apropos of nothing really - I thought I’d take the opportunity to do a simple simulation to briefly explore the likelihood&amp;hellip;</description>
    </item>
    
    <item>
      <title>Use CircleCI for R Projects</title>
      <link>/post/use-circleci-for-r-projects/</link>
      <pubDate>Wed, 18 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/use-circleci-for-r-projects/</guid>
      <description>Why CircleCI? Yes, I know using Travis CI is this easy, thanks to devtools&amp;hellip;</description>
    </item>
    
    <item>
      <title>Bayesian Estimation of Signal Detection Models, Part 3</title>
      <link>/post/bayesian-estimation-of-signal-/</link>
      <pubDate>Mon, 16 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-estimation-of-signal-/</guid>
      <description>Introduction Example data: Rating task EVSDT: one subject’s rating responses UVSDT: one subject’s rating responses References This post is the third part in a series of blog posts on Signal Detection models: In the first part, I described how to estimate the equal variance Gaussian SDT (EVSDT) model for a single participant, using Bayesian (generalized linear and nonlinear) modeling&amp;hellip;</description>
    </item>
    
    <item>
      <title>Can we use B-splines to generate non-linear data?</title>
      <link>/post/can-we-use-bsplines-to-generat/</link>
      <pubDate>Mon, 16 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/can-we-use-bsplines-to-generat/</guid>
      <description>I’m exploring the idea of adding a function or set of functions to the simstudy package that would make it possible to easily generate non-linear data. One way to do this would be using B-splines. Typically, one uses splines to fit a curve to data, but I thought it might be useful to switch things around a bit to use the underlying splines to generate data. This would facilitate exploring models where we know the assumption of linearity is&amp;hellip;</description>
    </item>
    
    <item>
      <title>Geocoding with R</title>
      <link>/post/geocoding-with-r/</link>
      <pubDate>Fri, 13 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/geocoding-with-r/</guid>
      <description>In the previous post I discussed some reasons to use R instead of Excel to analyze and visualize data and provided a brief introduction to the R programming language. That post used an example of letters sent to the sixteenth-century merchant Daniel van der Meulen in 1585. One aspect missing from the analysis was a geographical visualization of the&amp;hellip;</description>
    </item>
    
    <item>
      <title>A minor update to simstudy provides an excuse to talk a bit about the negative binomial and Poisson distributions</title>
      <link>/post/a-minor-update-to-simstudy-pro/</link>
      <pubDate>Thu, 05 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/a-minor-update-to-simstudy-pro/</guid>
      <description>I just updated simstudy to version 0.1.5 (available on CRAN ) so that it now includes several new distributions - exponential , discrete uniform , and negative binomial . As part of the release, I thought I’d explore the negative binomial just a bit, particularly as it relates to the Poisson distribution. The Poisson distribution is a discrete (integer) distribution of outcomes of non-negative values that is often used to describe count&amp;hellip;</description>
    </item>
    
    <item>
      <title>Regular Expression Searching within Shiny Selectize Objects</title>
      <link>/post/regular-expression-searching-w/</link>
      <pubDate>Tue, 26 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/regular-expression-searching-w/</guid>
      <description>regexSelect is a small package that uses Shiny modules to solve a problem in Shiny selectize objects - regular expression (regex) searching. You can quickly filter the values in the selectize object, while being able to add that new regex query to the selectize list. This is great for long lists, since you can return multiple item simultaneously without needing to endlessly click items in a list!&amp;hellip;</description>
    </item>
    
    <item>
      <title>Little&#39;s MCAR test at different sample sizes</title>
      <link>/post/littles-mcar-test-at-different/</link>
      <pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/littles-mcar-test-at-different/</guid>
      <description>TLDR: Little&amp;rsquo;s MCAR test is unable to tell data that are MCAR from data that are MAR in small samples, but maintains the nominal error rate when null is true across a wide range of sample sizes. I just found out about the R simglm package and decided to do a small simulation to test Little&amp;rsquo;s MCAR test1 under different sample sizes. I could have investigated heteroskedasticity in linear regression instead, and I probably will in the&amp;hellip;</description>
    </item>
    
    <item>
      <title>Theil-Sen regression in R</title>
      <link>/post/theilsen-regression-in-r/</link>
      <pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/theilsen-regression-in-r/</guid>
      <description>TLDR: When performing a simple linear regression, if you have any concern about outliers or heterosedasticity, consider the Theil-Sen estimator. A simple linear regression estimator that is not commonly used or taught in the social sciences is the Theil-Sen estimator. This is a shame given that this estimator is very intuitive, once you know what a slope&amp;hellip;</description>
    </item>
    
    <item>
      <title>A simstudy update provides an excuse to talk a little bit about latent class regression and the EM algorithm</title>
      <link>/post/a-simstudy-update-provides-an-/</link>
      <pubDate>Wed, 20 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/a-simstudy-update-provides-an-/</guid>
      <description>I was just going to make a quick announcement to let folks know that I’ve updated the simstudy package to version 0.1.4 (now available on CRAN) to include functions that allow conversion of columns to factors, creation of dummy variables, and most importantly, specification of outcomes that are more flexibly conditional on previously defined&amp;hellip;</description>
    </item>
    
    <item>
      <title>Hvordan klarer de politiske partier sig på Facebook?</title>
      <link>/post/hvordan-klarer-de-politiske-pa/</link>
      <pubDate>Tue, 19 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/hvordan-klarer-de-politiske-pa/</guid>
      <description>For ganske nyligt opdagede jeg, at der er lavet en RFacebook pakke til at trække data fra Facebooks Graph API. Det blev brugt til en analyse af Donald Trump og Hillary Clintons kampagner under den seneste amerikanske&amp;hellip;</description>
    </item>
    
    <item>
      <title>Linear regression with violation of heteroskedasticity with small samples</title>
      <link>/post/linear-regression-with-violati/</link>
      <pubDate>Tue, 19 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/linear-regression-with-violati/</guid>
      <description>TLDR: In small samples, the wild bootstrap implemented in the R hcci package is a good bet when heteroskedasticity is a concern. Today while teaching the multiple regression lab, I showed the class the standardized residuals versus standardized predictor plot SPSS lets you produce. It is the plot we typically use to assess homoskedasticity. The sample size for the analysis was 44. I mentioned how the regression slopes are fine under heteroskedasticity, but inference $(t, SE, pvalue)$ may be&amp;hellip;</description>
    </item>
    
    <item>
      <title>Automating roxygen2 package documentation</title>
      <link>/post/automating-roxygen2-package-do/</link>
      <pubDate>Mon, 18 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/automating-roxygen2-package-do/</guid>
      <description>Thinking of creating a new package? Dread the task of function documentation? Afraid to run devtools::check(build_args = &amp;lsquo;&amp;ndash;as-cran&amp;rsquo;) and get bombarded by Errors, Warnings, and Notes (oh my!) ? Wait&amp;hellip;. breathe! After feeling all that anxiety and following all of Hadley&amp;rsquo;s directions online, I felt I was doing a lot of manual labour, and that a package should be doing all lot of this for&amp;hellip;</description>
    </item>
    
    <item>
      <title>Pur(r)ify Your Carets</title>
      <link>/post/purrify-your-carets/</link>
      <pubDate>Sun, 17 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/purrify-your-carets/</guid>
      <description>The motivation An example using BostonHousing data Load libs &amp;amp; data Create a starter dataframe Select the models Create data-model combinations Solve the models Extract results In conclusion tl;dr: You’ll learn how to use purrr, caret and list-cols to quickly create hundreds of dataset + model combinations, store data &amp;amp; model objects neatly in one tibble, and post process programatically. These tools enable succinct functional programming in which a lot gets done with just a few lines of&amp;hellip;</description>
    </item>
    
    <item>
      <title>Complier average causal effect? Exploring what we learn from an RCT with participants who don&#39;t do what they are told</title>
      <link>/post/complier-average-causal-effect/</link>
      <pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/complier-average-causal-effect/</guid>
      <description>Inspired by a free online course titled Complier Average Causal Effects (CACE) Analysis and taught by Booil Jo and Elizabeth Stuart (through Johns Hopkins University), I’ve decided to explore the topic a little bit. My goal here isn’t to explain CACE analysis in extensive detail (you should definitely go take the course for that), but to describe the problem generally and then (of course) simulate some data. A plot of the simulated data gives a sense of what we are estimating and&amp;hellip;</description>
    </item>
    
    <item>
      <title>Further considerations of a hidden process underlying categorical responses</title>
      <link>/post/further-considerations-of-a-hi/</link>
      <pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/further-considerations-of-a-hi/</guid>
      <description>In my previous post , I described a continuous data generating process that can be used to generate discrete, categorical outcomes. In that post, I focused largely on binary outcomes and simple logistic regression just because things are always easier to follow when there are fewer moving parts. Here, I am going to focus on a situation where we have multiple outcomes, but with a slight twist - these groups of interest can be interpreted in an ordered&amp;hellip;</description>
    </item>
    
    <item>
      <title>A hidden process behind binary or other categorical outcomes?</title>
      <link>/post/a-hidden-process-behind-binary/</link>
      <pubDate>Mon, 28 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/a-hidden-process-behind-binary/</guid>
      <description>I was thinking a lot about proportional-odds cumulative logit models last fall while designing a study to evaluate an intervention’s effect on meat consumption. After a fairly extensive pilot study, we had determined that participants can have quite a difficult time recalling precise quantities of meat consumption, so we were forced to move to a categorical&amp;hellip;</description>
    </item>
    
    <item>
      <title>OSGeo-Live Project</title>
      <link>/post/osgeolive-project/</link>
      <pubDate>Thu, 24 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/osgeolive-project/</guid>
      <description>OSGeo-Live is a self-containing ISO with around 50 FOSS dedicated to GIS. A virtual macine archive is also provided. I join the project in November (or early December) 2015, as I was already a librist when I was a became a GIS technician. As I started a Bachelor degree in GIS, I need GIS tools to do assingnements, and they often needed FOSS like QGIS and/or PostgreSQL/PostGIS, or a&amp;hellip;</description>
    </item>
    
    <item>
      <title>Be careful not to control for a post-exposure covariate</title>
      <link>/post/be-careful-not-to-control-for-/</link>
      <pubDate>Mon, 21 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/be-careful-not-to-control-for-/</guid>
      <description>A researcher was presenting an analysis of the impact various types of childhood trauma might have on subsequent substance abuse in adulthood. Obviously, a very interesting and challenging research question. The statistical model included adjustments for several factors that are plausible confounders of the relationship between trauma and substance use, such as childhood&amp;hellip;</description>
    </item>
    
    <item>
      <title>My Approach to Digital Humanities</title>
      <link>/post/my-approach-to-digital-humanit/</link>
      <pubDate>Fri, 18 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/my-approach-to-digital-humanit/</guid>
      <description>Digital humanities holds the promise of increasing the means by which scholars are able to analyze and present data. Though some sentiments about the significance of digital humanities might be overblown, there is no doubt that the more ways we have to analyze sources the better . Learning a variety of the tools that make up the rather nebulous universe of digital humanities is like learning a new&amp;hellip;</description>
    </item>
    
    <item>
      <title>New kinds of Projects</title>
      <link>/post/new-kinds-of-projects/</link>
      <pubDate>Wed, 16 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/new-kinds-of-projects/</guid>
      <description>In the process of learning about how I could use digital technologies to better organize my research, I quickly started to think about how I might extend these skills to produce new kinds of outputs. 1 I was familiar with the concept of digital humanities, but the step from an internal process of organizing research and writing to production seemed both too nebulous and difficult. Digital humanities also seemed to concentrate on the&amp;hellip;</description>
    </item>
    
    <item>
      <title>On the interpretation of regression coefficients</title>
      <link>/post/on-the-interpretation-of-regre/</link>
      <pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/on-the-interpretation-of-regre/</guid>
      <description>TLDR: We should interpret regression coefficients for continuous variables as we would descriptive dummy variables, unless we intend to make causal claims. I am going to be teaching regression labs in the Fall, and somehow, I stumbled onto Gelman and Hill&amp;rsquo;s Data analysis using regression and multilevel/hierarchical models.1 So I started reading it and it&amp;rsquo;s a good book. A useful piece of advice they give is to interpret regression coefficients in a predictive manner&amp;hellip;</description>
    </item>
    
    <item>
      <title>Combining R and Python for data analysis</title>
      <link>/post/combining-r-and-python-for-dat/</link>
      <pubDate>Fri, 04 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/combining-r-and-python-for-dat/</guid>
      <description>As part of my PhD work I characterise nanomaterials using Energy-dispersive X-ray spectroscopy (EDX) in a Scanning Transmission Electron Microscope. We do this to obtain spatial information about the chemical composition of a sample on the nanoscale. Basically, an image is obtained by raster-scanning the electron beam and recording an X-ray spectrum in each position. This effectively gives a 3-dimensional dataset, where for each pixel a full spectrum is&amp;hellip;</description>
    </item>
    
    <item>
      <title>How would you bet? Lessons from a Biased Coin Flipping Experiment</title>
      <link>/post/how-would-you-bet-lessons-from/</link>
      <pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/how-would-you-bet-lessons-from/</guid>
      <description>Recently I listened to a podcast featuring Victor Haghani of Elm Partners , who described a fascinating coin-flipping experiment. The experiment was designed to be played for 30 minutes by participants in groups of 2-15 in university classrooms or office conference rooms, without consulting each other or the internet or other resources. To conduct the experiment, a custom web app was built for placing bets on a simulated coin with a 60% chance of coming up&amp;hellip;</description>
    </item>
    
    <item>
      <title>A guide to awesome slides</title>
      <link>/post/a-guide-to-awesome-slides/</link>
      <pubDate>Mon, 31 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/a-guide-to-awesome-slides/</guid>
      <description>Recently on Twitter I saw Jenny Bryan post this guide to slide design by Melinda Seckington If this spares me but one beamer presentation, it is worth it 😬 The Art of Slide Design @mseckington ht @sjackman https://t.co/EWOsJyzbKZ - Jenny Bryan (@JennyBryan) July 29,&amp;hellip;</description>
    </item>
    
    <item>
      <title>Análisis de Varianza (ANOVA)</title>
      <link>/post/an%C3%A1lisis-de-varianza-anova/</link>
      <pubDate>Mon, 31 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/an%C3%A1lisis-de-varianza-anova/</guid>
      <description>Curso intersemestral impartido en el verano de 2017 por&amp;hellip;</description>
    </item>
    
    <item>
      <title>The new MODIStsp website (based on pkgdown) is online !</title>
      <link>/post/the-new-modistsp-website-based/</link>
      <pubDate>Tue, 25 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/the-new-modistsp-website-based/</guid>
      <description>The MODIStsp website, which lay abandoned since several months on github pages, recently underwent a major overhaul thanks to&amp;hellip;</description>
    </item>
    
    <item>
      <title>Correlated Psychological Variables, Uncertainty, and Bayesian Estimation</title>
      <link>/post/correlated-psychological-varia/</link>
      <pubDate>Tue, 18 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/correlated-psychological-varia/</guid>
      <description>Psychological Variables Example data Possible models Independent Generalized Linear Models Hierarchical GLM Estimating GLMM with Maximum Likelihood Methods Bayesian estimation of GLMM Comparing models Conclusion Update References Assessing the correlations between psychological variabless, such as abilities and improvements, is one essential goal of psychological&amp;hellip;</description>
    </item>
    
    <item>
      <title>Visualizing varying effects&#39; posteriors with joyplots</title>
      <link>/post/visualizing-varying-effects-po/</link>
      <pubDate>Thu, 13 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/visualizing-varying-effects-po/</guid>
      <description>Joy Plots ggjoy References Joy Plots In a previous blog post , I introduced a helper function for drawing “better” forest plots from Bayesian meta-analyses estimated with the brms R package (Buerkner&amp;hellip;</description>
    </item>
    
    <item>
      <title>Open Science tools for our research group</title>
      <link>/post/open-science-tools-for-our-res/</link>
      <pubDate>Mon, 10 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/open-science-tools-for-our-res/</guid>
      <description>Currently in the Organic Surface Chemistry group , there are large variations between our group members when it comes to the tools used for data analysis. Some people feel most comfortable in spreadsheet programs such as Origin or Excel, while others rely on a mix of Matlab, R, Python or other tools. We use a lot of different experimental techniques in our research, and therefore generate data in a lot of different&amp;hellip;</description>
    </item>
    
    <item>
      <title>Using simulation for power analysis</title>
      <link>/post/using-simulation-for-power-ana/</link>
      <pubDate>Mon, 10 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/using-simulation-for-power-ana/</guid>
      <description>Simulation can be super helpful for estimating power or sample size requirements when the study design is complex. This approach has some advantages over an analytic one (i.e. one based on a formula), particularly the flexibility it affords in setting up the specific assumptions in the planned study, such as time trends, patterns of missingness, or effects of different levels of&amp;hellip;</description>
    </item>
    
    <item>
      <title>simstudy update</title>
      <link>/post/simstudy-update/</link>
      <pubDate>Wed, 05 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/simstudy-update/</guid>
      <description>In an earlier post , I described in a fair amount of detail an algorithm to generate correlated binary or Poisson data. I mentioned that I would be updating simstudy with functions that would make generating these kind of data relatively painless. Well, I have managed to do that, and the updated package (version 0.1.3) is available for download from&amp;hellip;</description>
    </item>
    
    <item>
      <title>First look at Tidycensus</title>
      <link>/post/first-look-at-tidycensus/</link>
      <pubDate>Fri, 30 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/first-look-at-tidycensus/</guid>
      <description>The whole future of the US census has been coming under scrutiny recently, but, thankfully, we are getting more tools to scrutinise both its decennial data and that of its sister-source, the American Community service (ACS). Specifically, Kyle Walker’s tidycensus and tigris packages which return data-frames (including shape-files as list-columns, if required) from the census API and Edzer Pebesma’s sf, simple features,&amp;hellip;</description>
    </item>
    
    <item>
      <title>Investment Strategy Diversification</title>
      <link>/post/investment-strategy-diversific/</link>
      <pubDate>Tue, 27 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/investment-strategy-diversific/</guid>
      <description>By all conventional and even some unconventional measures, the US stock market is trading way beyond historical valuation averages and is closer to all time highs. Passive stock index investors have enjoyed a period of extraordinary gains in one of the longest running bull markets. No other major asset class has come close in the last 7&amp;hellip;</description>
    </item>
    
    <item>
      <title>Balancing on multiple factors when the sample is too small to stratify </title>
      <link>/post/balancing-on-multiple-factors-/</link>
      <pubDate>Mon, 26 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/balancing-on-multiple-factors-/</guid>
      <description>Ideally, a study that uses randomization provides a balance of characteristics that might be associated with the outcome being studied. This way, we can be more confident that any differences in outcomes between the groups are due to the group assignments and not to differences in characteristics. Unfortunately, randomization does not guarantee balance, especially with smaller sample&amp;hellip;</description>
    </item>
    
    <item>
      <title>Thinking about Workflow</title>
      <link>/post/thinking-about-workflow/</link>
      <pubDate>Thu, 22 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/thinking-about-workflow/</guid>
      <description>In the spring of 2011, I was in the middle of doing research for my dissertation. I had recently returned from my second extended trip to the archives in the Netherlands and Belgium and had accumulated a ton of notes. I knew that technology had drastically altered the possibilities for research, but the fundamentals of my own workflow were hardly different than they had been when I began undergrad in the early&amp;hellip;</description>
    </item>
    
    <item>
      <title>Copulas and correlated data generation</title>
      <link>/post/copulas-and-correlated-data-ge/</link>
      <pubDate>Mon, 19 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/copulas-and-correlated-data-ge/</guid>
      <description>Using the simstudy package, it’s possible to generate correlated data from a normal distribution using the function genCorData . I’ve wanted to extend the functionality so that we can generate correlated data from other sorts of distributions; I thought it would be a good idea to begin with binary and Poisson distributed data, since those come up so frequently in my&amp;hellip;</description>
    </item>
    
    <item>
      <title>When marginal and conditional logistic model estimates diverge</title>
      <link>/post/when-marginal-and-conditional-/</link>
      <pubDate>Fri, 09 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/when-marginal-and-conditional-/</guid>
      <description>Say we have an intervention that is assigned at a group or cluster level but the outcome is measured at an individual level (e.g. students in different schools, eyes on different individuals). And, say this outcome is binary; that is, something happens, or it&amp;hellip;</description>
    </item>
    
    <item>
      <title>Animated Plots As Part Of Exploratory Data Analysis</title>
      <link>/post/animated-plots-as-part-of-expl/</link>
      <pubDate>Tue, 30 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/animated-plots-as-part-of-expl/</guid>
      <description>The internet seems to be booming with blog posts on animated graphs, whether it&amp;rsquo;s for more serious purposes or not so much. I didn&amp;rsquo;t think anything more of it than just a gimmick or a cool way of spicing up your conference&amp;hellip;</description>
    </item>
    
    <item>
      <title>Luck in Rebalance Timing</title>
      <link>/post/luck-in-rebalance-timing/</link>
      <pubDate>Mon, 29 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/luck-in-rebalance-timing/</guid>
      <description>An important and often overlooked topic was raised by Corey Hoffstein at NewFound Research. Here are his first couple of tweets on that&amp;hellip;</description>
    </item>
    
    <item>
      <title>It can be easy to explore data generating mechanisms with the simstudy package</title>
      <link>/post/it-can-be-easy-to-explore-data/</link>
      <pubDate>Tue, 16 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/it-can-be-easy-to-explore-data/</guid>
      <description>I learned statistics and probability by simulating data. Sure, I did the occasional proof, but I never believed the results until I saw it in a simulation. I guess I have it backwards, but I that’s just the way I am. And now that I am a so-called professional, I continue to use simulation to understand models, to do sample size estimates and power calculations, and of course to teach. Sure - I’ll use the occasional formula when one exists, but I always feel the need to check it with&amp;hellip;</description>
    </item>
    
    <item>
      <title>Everyone knows that loops in R are to be avoided, but vectorization is not always possible</title>
      <link>/post/everyone-knows-that-loops-in-r/</link>
      <pubDate>Wed, 10 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/everyone-knows-that-loops-in-r/</guid>
      <description>It goes without saying that there are always many ways to solve a problem in R, but clearly some ways are better (for example, faster) than others. Recently, I found myself in a situation where I could not find a way to avoid using a loop, and I was immediately concerned, knowing that I would want this code to be flexible enough to run with a very large number of observations, possibly over many observations.</description>
    </item>
    
    <item>
      <title>You should make an R package for your paper</title>
      <link>/post/you-should-make-an-r-package-f/</link>
      <pubDate>Wed, 10 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/you-should-make-an-r-package-f/</guid>
      <description>Introduction I had wanted to get into R package creation for a while now, but finally got a chance to do so for my lab’s recent Zika paper, Assessing Real-time Zika Risk in the United States . If you’re interested in learning about the content from that paper you can check out the blogpost hosted on the BMC Infectious Diseases blog . I’ve also given an introduction to the package on the rtZIKVrisk&amp;hellip;</description>
    </item>
    
    <item>
      <title>MODIStsp (v 1.3.2) is on CRAN !</title>
      <link>/post/modistsp-v-132-is-on-cran-/</link>
      <pubDate>Tue, 18 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/modistsp-v-132-is-on-cran-/</guid>
      <description>We are glad to report that MODIStsp is now also available on CRAN ! From now on, you can therefore install it by simply using: install.packages(&amp;ldquo;MODIStsp&amp;rdquo;) In v 1.3.2 we also added the functionality to automatically apply scale and offset coefficients on MODIS original values according with the specifications of single MODIS&amp;hellip;</description>
    </item>
    
    <item>
      <title>slickR</title>
      <link>/post/slickr/</link>
      <pubDate>Fri, 07 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/slickr/</guid>
      <description>We are happy to bring the slick JavaScript library to R. It is self described as &amp;ldquo;the last carousel you&amp;rsquo;ll ever need&amp;rdquo;. This carousel is based on putting the elements of the carousel in a div HTML tag. This makes the carousel very versatile in what can be placed&amp;hellip;</description>
    </item>
    
    <item>
      <title>What&#39;s News?</title>
      <link>/post/whats-news/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/whats-news/</guid>
      <description>newsflash packagePolymath, Bob Rudis (aka hrbrmstr) has recently released the newsflash package which is a “set of tools to Work with the Internet Archive and GDELT Television Explorer”&amp;rdquo; In a recent blog post, based on a gdelt project creator article, he details the coverage of Hillary Clinton’s email server woes, with the, unsurprising, fact that FOX News spent more time on the issue than other&amp;hellip;</description>
    </item>
    
    <item>
      <title>ggedit 0.2.0</title>
      <link>/post/ggedit-020/</link>
      <pubDate>Fri, 31 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/ggedit-020/</guid>
      <description>We are pleased to announce the release of the ggedit package on CRAN . To install the package you can call the standard R&amp;hellip;</description>
    </item>
    
    <item>
      <title>Parallel benchmarking with OpenML and mlr</title>
      <link>/post/parallel-benchmarking-with-ope/</link>
      <pubDate>Wed, 22 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/parallel-benchmarking-with-ope/</guid>
      <description>With this post I want to show you how to benchmark several learners (or learners with different parameter settings) using several data sets in a structured and parallelized fashion. For this we want to use batchtools. The data that we will use here is stored on the open machine learning platform openml.org and we can download it together with information on what to do with it in form of a&amp;hellip;</description>
    </item>
    
    <item>
      <title>ggedit 0.1.1</title>
      <link>/post/ggedit-011/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/ggedit-011/</guid>
      <description>ggedit is a package that lets users interactively edit ggplot layer and theme aesthetics. In a previous post we showed you how to use it in a collaborative workflow using standard R scripts. More importantly, we highlighted that ggedit outputs to the user, after editing, updated: gg plots, layers, scales and themes as both self-contained objects and script that you can paste directly in your code&amp;hellip;.</description>
    </item>
    
    <item>
      <title>Finite Mixture Modeling using Flexmix</title>
      <link>/post/finite-mixture-modeling-using-/</link>
      <pubDate>Wed, 01 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/finite-mixture-modeling-using-/</guid>
      <description>Model Based Clustering Quick EDA Model building Mixtures of Regressions Quick EDA Model Building Results Further investigation Notes References This page replicates the codes written by Grun &amp;amp; Leish (2007) in ‘FlexMix: An R package for finite mixture modelling’, University of Wollongong, Australia. My intent here was to learn the flexmix package by replicating the results by the authors. Model Based Clustering The model based clustering on the whiskey&amp;hellip;</description>
    </item>
    
    <item>
      <title>Magic reprex</title>
      <link>/post/magic-reprex/</link>
      <pubDate>Wed, 11 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/magic-reprex/</guid>
      <description>Making reproducible examples can be hard. There&amp;rsquo;s a lot of things you need to consider. Like, making sure your environment is clean, the right packages are loaded, the code is formatted nicely, and images are the right resolution and dimension. Getting all of these ducks lined up can sometimes take a couple of minutes, if you have a nice tightly defined problem. Other times, it can take much, much&amp;hellip;</description>
    </item>
    
    <item>
      <title>Empirical methods workshop</title>
      <link>/post/empirical-methods-workshop/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/empirical-methods-workshop/</guid>
      <description>Various sessions introducing tools for empirical projects. Slides and additional links available on Github&amp;hellip;</description>
    </item>
    
  </channel>
</rss>