<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bgstiebergithubio on rscribers</title>
    <link>/tags/bgstiebergithubio/</link>
    <description>Recent content in Bgstiebergithubio on rscribers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 06 Aug 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/bgstiebergithubio/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Everything I Know About Machine Learning I Learned from Making Soup</title>
      <link>/post/everything-i-know-about-machin/</link>
      <pubDate>Mon, 06 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/everything-i-know-about-machin/</guid>
      <description>Introduction In this post, I’m going to make the claim that we can simplify some parts of the machine learning process by using the analogy of making soup. I think this analogy can improve how a data scientist explains machine learning to a broad audience, and it provides a helpful framework throughout the model building&amp;hellip;</description>
    </item>
    
    <item>
      <title>Golf, Tidy Data, and Using Data Analysis to Guide Strategy</title>
      <link>/post/golf-tidy-data-and-using-data-/</link>
      <pubDate>Sun, 24 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/golf-tidy-data-and-using-data-/</guid>
      <description>Introduction I’m going to use this post to discuss some of the aspects of data science that interest me most (tidy data as well as using data to guide strategy). I’ll be discussing these topics through the lens of a data analysis of results from a few high school golf tournaments. I’m going to take a little bit of time to talk about tidy data. When I scraped the data used for this analysis, it wasn’t really stored in a tidy format, and there’s a good reason for&amp;hellip;</description>
    </item>
    
    <item>
      <title>An Introduction to the kmeans Algorithm</title>
      <link>/post/an-introduction-to-the-kmeans-/</link>
      <pubDate>Mon, 28 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/an-introduction-to-the-kmeans-/</guid>
      <description>This post will provide an R code-heavy, math-light introduction to selecting the (k) in k means. It presents the main idea of kmeans, demonstrates how to fit a kmeans in R, provides some components of the kmeans fit, and displays some methods for selecting k. In addition, the post provides some helpful functions which may make fitting kmeans a bit easier. kmeans clustering is an example of unsupervised learning, where we do not have an output we’re explicitly trying to&amp;hellip;</description>
    </item>
    
    <item>
      <title>My First Post</title>
      <link>/post/my-first-post/</link>
      <pubDate>Sat, 26 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/my-first-post/</guid>
      <description>Welcome to my blog! I plan to use this website to present data explorations and analyses in a way that’s understandable to a broad audience. I hope to demonstrate the utility of applying ideas like machine learning, data visualization, and exploratory data analysis to day-to-day life to improve decision-making processes. I was inspired to create a blog after reading this post by David Robinson. New blog post: &amp;ldquo;Advice to aspiring data scientists: start a blog&amp;rdquo;&amp;hellip;</description>
    </item>
    
  </channel>
</rss>