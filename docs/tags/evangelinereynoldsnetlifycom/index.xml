<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Evangelinereynoldsnetlifycom on rscribers</title>
    <link>/tags/evangelinereynoldsnetlifycom/</link>
    <description>Recent content in Evangelinereynoldsnetlifycom on rscribers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Sep 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/evangelinereynoldsnetlifycom/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Slow ggplot</title>
      <link>/post/slow-ggplot/</link>
      <pubDate>Thu, 13 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/slow-ggplot/</guid>
      <description>This post has lots in common with previous posts on “the layered presentation of graphics”. It is about building up plots, but now with a focus on this incramental change for teaching ggplot2. The rational is that observing the cause and effect of incremental change is easier to digest, and that the repetition in this approach means students have more chances to learn the ggplot2 functions. My recent tweet presented the technique: Here, building up a #ggplot2 as slowly as possible,&amp;hellip;</description>
    </item>
    
    <item>
      <title>Layered Presentation of Graphics, revised</title>
      <link>/post/layered-presentation-of-graphi/</link>
      <pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/layered-presentation-of-graphi/</guid>
      <description>Here is my second post is about how to implement a layered presentation of a graphics . My previous implementation used the alpha transparency aesthetic to hide all but one point. But, now, rethink things, now for the 3rd time or so, I just subset the data associated with the first geom layer, leaving the global data complete. I think it is more straight forward than messing around with&amp;hellip;</description>
    </item>
    
    <item>
      <title>Where should you declare aesthetics?  Globally, or geom-by-geom?</title>
      <link>/post/where-should-you-declare-aesth/</link>
      <pubDate>Mon, 16 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/where-should-you-declare-aesth/</guid>
      <description>Where should you declare aesthetics? Globally or in the geom_*() function? The answer to this question, in some sense is personal preference, because there are simply different ways to get the same job done in the ggplot architecture. My preference is declaring all aesthetic mappings as global unless there are conflicts. Below is an example that, I hope, will persuade you to my preference. We graph the increase in life expectancy by year for three&amp;hellip;</description>
    </item>
    
    <item>
      <title>Tuition Increases for Tidy Tuesday</title>
      <link>/post/tuition-increases-for-tidy-tue/</link>
      <pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tuition-increases-for-tidy-tue/</guid>
      <description>Here’s the code for my first Tidy Tuesday&amp;hellip;</description>
    </item>
    
    <item>
      <title>Wide data to long using the tidyverse (tidyr&#39;s gather function)</title>
      <link>/post/wide-data-to-long-using-the-ti/</link>
      <pubDate>Fri, 06 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/wide-data-to-long-using-the-ti/</guid>
      <description>A wide data storage format is an efficient and compact way to store information. And this organization perhaps it makes data easier to inspect. We have wide monitors our laptops and destops. However, for visualization and analysis you generally need to transform this data from the wide format to a “tidy”, long format. We look at the case where just one variable is stored in a&amp;hellip;</description>
    </item>
    
    <item>
      <title>Selection effects</title>
      <link>/post/selection-effects/</link>
      <pubDate>Mon, 25 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/selection-effects/</guid>
      <description>Saw this tweet by @Nolan_Mc , and thought, this is good fodder for a quick blog post – a true one-hour one-off. Run this Stata code, and then tell me the logic is sound set obs 20000 gen x =runiform() gen y = x + .1*rnormal() corr x y corr x y if x &amp;gt; .95 https://t.co/z5bE0iAH3I - Nolan McCarty (@Nolan_Mc) June 17,&amp;hellip;</description>
    </item>
    
    <item>
      <title>The visual taming of a paradox</title>
      <link>/post/the-visual-taming-of-a-paradox/</link>
      <pubDate>Tue, 19 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/the-visual-taming-of-a-paradox/</guid>
      <description>@drob has posted code to play with on Twitter today. To illustrate what he calls a veridical paradox he’s posted the set up, the code and result of a coin flipping experiment: A #tidyverse simulation to demonstrate that if you wait for two heads in a row, it takes 6 flips on average, while you wait for a heads then a tails, it takes 4 flips on average h/t @CutTheKnotMath #rstats pic.</description>
    </item>
    
    <item>
      <title>Federalist Papers</title>
      <link>/post/federalist-papers/</link>
      <pubDate>Sat, 16 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/federalist-papers/</guid>
      <description>Every couple of weeks I like to explore data that’s brand new to me. I anticipate a one-hour, one-off project. Usually this turns out to be a beautiful lie, and the projects chew up much more time. Still, this enticing time-line is pulling me into new projects from time to time. Earlier this week, I heard about the dispute of authorship of some of the Federalist papers. &amp;ldquo;Hamilton wrote the other 51!</description>
    </item>
    
    <item>
      <title>Covariance -- A Visual Walk Through</title>
      <link>/post/covariance-a-visual-walk-throu/</link>
      <pubDate>Thu, 14 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/covariance-a-visual-walk-throu/</guid>
      <description>In a previous post, I’ve looked at walking through the calculation of variance and standard deviation, visualizing each&amp;hellip;</description>
    </item>
    
    <item>
      <title>Eat near the Big Ben?  That will cost you...</title>
      <link>/post/eat-near-the-big-ben-that-will/</link>
      <pubDate>Tue, 12 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/eat-near-the-big-ben-that-will/</guid>
      <description>What’s the cost of eating near an iconic location? We all might suspect eating in a fun location is pricier. I used the menu and restaurant location data that #MakeoverMonday “assigned” one week last December to demonstrate this among London Wetherspoon Pubs . #MakeoverMonday is a fun data visualization initiative; most participants use Tableau as their preferred visualization tool. But I’ve used R and ggplot() and the organizers and participants have been very&amp;hellip;</description>
    </item>
    
  </channel>
</rss>