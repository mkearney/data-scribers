<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Modeling on DATA SCrIbers</title>
    <link>/tags/modeling/</link>
    <description>Recent content in Modeling on DATA SCrIbers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Sep 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/modeling/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Data Science in Mental Health</title>
      <link>/post/data-science-in-mental-health/</link>
      <pubDate>Thu, 20 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/data-science-in-mental-health/</guid>
      <description>I came across two articles recently that I thought spoke to each other in an interesting way. The first was a New York Times piece about the failings of data science firms who try to identify school shootings before they happen by social media posts. The second was a Vox article about how a crisis counseling hotline successfully used data science to flag callers who are at higher risk of suicide or&amp;hellip;</description>
    </item>
    
    <item>
      <title>#GI2018 - Day Three</title>
      <link>/post/gi2018-day-three/</link>
      <pubDate>Wed, 19 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/gi2018-day-three/</guid>
      <description>Typos everywhere. Things may change dramatically over time as I scan back through notes. I’ve tried to respect #notwitter. Will be updated&amp;hellip;</description>
    </item>
    
    <item>
      <title>Parallel Computing in R</title>
      <link>/post/parallel-computing-in-r/</link>
      <pubDate>Wed, 19 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/parallel-computing-in-r/</guid>
      <description>We&amp;rsquo;re excited to host Jared Lander, Chief Data Scientist of Lander Analytics, the organizer of the New York Open Statistical Programming Meetup and the New York R Conference, and author of R for Everyone, to talk about parallel computing in&amp;hellip;</description>
    </item>
    
    <item>
      <title>#GI2018 - Day Two</title>
      <link>/post/gi2018-day-two/</link>
      <pubDate>Tue, 18 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/gi2018-day-two/</guid>
      <description>Typos everywhere. Things may change dramatically over time as I scan back through notes. I’ve tried to respect #notwitter. Will be updated&amp;hellip;</description>
    </item>
    
    <item>
      <title>Introducing debkeepr</title>
      <link>/post/introducing-debkeepr/</link>
      <pubDate>Tue, 18 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/introducing-debkeepr/</guid>
      <description>The economic historian encounters the difficulties of handling non-decimal currencies in two main contexts. In reading through documents that may or may not be primarily economic in nature, the researcher comes across sets of values that need to be manipulated to better understand their&amp;hellip;</description>
    </item>
    
    <item>
      <title>Building Infrastructure with R</title>
      <link>/post/building-infrastructure-with-r/</link>
      <pubDate>Mon, 17 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/building-infrastructure-with-r/</guid>
      <description>For this event we will explore how to build tools and infrastructure with R. 6:15-6:30pm Introductions and Social 6:30- 6:45 pm NYT announcements (Data, Tech, HR) 6:45-6:55pm R-Ladies New York Announcements 6:55-7:25pm Object Oriented Programming in R 7:25-7:55pm Big Data in R with Small Prototypes: Scaling 8:00-8:15pm Networking. In this talk, Soumya will provide an introduction to using these programming techniques in workflows and project&amp;hellip;</description>
    </item>
    
    <item>
      <title>next up anova</title>
      <link>/post/next-up-anova/</link>
      <pubDate>Wed, 12 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/next-up-anova/</guid>
      <description>Next I need learn how to conduct ANOVA in R. the formula- specify which variable is your outcome and which are your grouping variables the data- which dataframe are you analysing In a clinical trial where you are looking to see if the drug improved mood scores you might specify&amp;hellip; As always,you can shortcut that by saying Or by specifying which variables to analyse using&amp;hellip;</description>
    </item>
    
    <item>
      <title>Binary, beta, beta-binomial</title>
      <link>/post/binary-beta-betabinomial/</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/binary-beta-betabinomial/</guid>
      <description>A couple of interesting things to note here. First is that the coefficient estimates are pretty similar to the beta regression model. However, the standard errors are slightly higher, as they should be, since we are using only observed probabilities and not the true (albeit randomly selected or generated) probabilities. So, there is another level of uncertainty beyond sampling error. With individual level binary outcomes (as opposed to count data we were working with before), GEE models are&amp;hellip;</description>
    </item>
    
    <item>
      <title>Some books I read in August</title>
      <link>/post/some-books-i-read-in-august/</link>
      <pubDate>Thu, 30 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/some-books-i-read-in-august/</guid>
      <description>October - China Mieville China Mieville is a very good science fiction writer, so I was intrigued when I saw that he wrote a book about the Russian revolution of 1917. Except for the first chapter that quickly covers the history leading up to the revolution and the final epilogue chapter, the book is organized so that each chapter represents a month starting from the overthrow of the Tsar in February to October when Lenin and the Bolsheviks take power (plus an&amp;hellip;</description>
    </item>
    
    <item>
      <title>How does Collinearity Influence Linear Regressions?</title>
      <link>/post/how-does-collinearity-influenc/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/how-does-collinearity-influenc/</guid>
      <description>This is a short simulation study trying to figure out the impact of collinearity on linear regressions. Load the necessary packages First, I write a little function to simulate collinearity. Draw data from function and save it. Now, consider the following linear regression: y ~ x1 +&amp;hellip;</description>
    </item>
    
    <item>
      <title>My first gganimate - exploring concepts from first year linear modelling!</title>
      <link>/post/my-first-gganimate-exploring-c/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/my-first-gganimate-exploring-c/</guid>
      <description>Have you ever had one of those moments whilst teaching where the content blows your mind? Today, whilst teaching MATH1005 at the University of Sydney, that exact thing happened to me. This weeks content was focused on teaching the students the introductions to linear modelling. A very strightforward topic, and one ususally understood well by first years. However, a twist in our teaching style left me with such an appreciation for the simpler explanations in&amp;hellip;</description>
    </item>
    
    <item>
      <title>The power of stepped-wedge designs</title>
      <link>/post/the-power-of-steppedwedge-desi/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/the-power-of-steppedwedge-desi/</guid>
      <description>The most important assumption I am making, however, is that the investigators can introduce the intervention at a small number of sites during each time period (for example, because the intervention involves extensive training and there is a limited number of trainers.) In this case, I am assuming that at most 10 sites can start the intervention at any point in time, and we must wait at least 4 weeks until the next wave can be&amp;hellip;</description>
    </item>
    
    <item>
      <title>Exploring London Crime with R heat maps</title>
      <link>/post/exploring-london-crime-with-r-/</link>
      <pubDate>Sun, 26 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/exploring-london-crime-with-r-/</guid>
      <description>Here’s a sweet collection of packages required to run this analysis: First thing&amp;hellip;</description>
    </item>
    
    <item>
      <title>Record linkage</title>
      <link>/post/record-linkage/</link>
      <pubDate>Sat, 25 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/record-linkage/</guid>
      <description>I recently encountered a problem that had a surprisingly elegant solution. I struggled a lot with solving this issue, so hopefully in writing this post I can save someone else the trouble! For reasons that are irrelevant, I wanted to track the performance of youth fencers across time. National ranking lists are posted each year, but the fencers’ names frequently change from year to&amp;hellip;</description>
    </item>
    
    <item>
      <title>Test Driven Infrastructure</title>
      <link>/post/test-driven-infrastructure/</link>
      <pubDate>Fri, 24 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/test-driven-infrastructure/</guid>
      <description>Software development has embraced techniques like TDD (Test Driven Development) to help reduce the cycle time between developing code and validating it works. As application development practice evolved, we needed to respond to change faster while still maintaining our quality - the way we developed our solutions needed to change - and so did our tools. Now that we have Infrastructure as Code (IaC) a whole new range of cycle time challenges have&amp;hellip;</description>
    </item>
    
    <item>
      <title>It&#39;s Alive! First Evidence that IBI VizEdit Works</title>
      <link>/post/its-alive-first-evidence-that-/</link>
      <pubDate>Thu, 16 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/its-alive-first-evidence-that-/</guid>
      <description>It is official. The program I have spent the better part of a year working on, the very centerpiece of my dissertation, works. Or at least, early indicators are in, and based on 22 cases, some of which required a great deal of manual editing, the program is returning estimates in line with expectations. Backing up, as I trip a little over my excitement, IBI VizEdit is an Rshiny application I created to help our lab process and edit heart rate&amp;hellip;</description>
    </item>
    
    <item>
      <title>How to Disable SELinux on CentOS 7</title>
      <link>/post/how-to-disable-selinux-on-cent/</link>
      <pubDate>Tue, 14 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/how-to-disable-selinux-on-cent/</guid>
      <description>SELinux (Security Enhanced Linux) is a Linux kernel security module that allows administrators and users more control over access controls. It allows access based on SELinux policy rules. SELinux policy rules specify how processes and users interact with each other as well as how processes and users interact with files. If there is no SELinux policy rule that specifically allows access, such as for a process opening a file, access is&amp;hellip;</description>
    </item>
    
    <item>
      <title>Tools for getting started with your PhD</title>
      <link>/post/tools-for-getting-started-with/</link>
      <pubDate>Thu, 09 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tools-for-getting-started-with/</guid>
      <description>What do I mean with this? Well, see for yourself whether you recognize any of the following behaviors: However, if you left all of those behaviors behind you long ago, well, you can close this tab and save yourself ten minutes. Alright, enough disclaiming, here we go, in no particular order: This one might easily be the one tool that saves you the most&amp;hellip;</description>
    </item>
    
    <item>
      <title>Topic Modelling of Trustpilot Reviews with R and tidytext</title>
      <link>/post/topic-modelling-of-trustpilot-/</link>
      <pubDate>Thu, 09 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/topic-modelling-of-trustpilot-/</guid>
      <description>Improving the look of figures in ggplot2 is fairly simple. For consistency, we’ll create a clean and simple theme based on the APA theme from the jtools package and change some of the features. The background colour will be set to a light grey hue. The grid lines are omitted in the APA theme. Given that our reviews are in Danish, we can use the happyorsad package to compute a sentiment score for each review.</description>
    </item>
    
    <item>
      <title>Reading vintage magazines with `hocr`</title>
      <link>/post/reading-vintage-magazines-with/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/reading-vintage-magazines-with/</guid>
      <description>library(tidyverse) library(tesseract) library(pdftools) library(hocr) library(here) library(fs) library(hunspell) library(hrbrthemes) library(patchwork) Challenge This post is inspired by recent tweet by Paige Bailey about vintage computer magazines made available for free download on archive.org. A number of people picked up on the idea of checking out some of the old magazines from the time they can remeber starting with&amp;hellip;</description>
    </item>
    
    <item>
      <title>Some Books I Read in July</title>
      <link>/post/some-books-i-read-in-july/</link>
      <pubDate>Mon, 23 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/some-books-i-read-in-july/</guid>
      <description>The Dilemmas of Lenin: Terrorism, War, Empire, Love, Revolution by Tariq Ali A very interesting biography of Lenin. The book isn’t a traditional biography. Instead, it’s a kind of intellectual biography focused around particular topics. Some of the topics, Bolshevik military strategy during the Civil War for instance, don’t mention Lenin at all. Additionally there are chapters on terrorism in Tsarist Russia, and the work of feminist&amp;hellip;</description>
    </item>
    
    <item>
      <title>Curl Command Examples</title>
      <link>/post/curl-command-examples/</link>
      <pubDate>Tue, 17 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/curl-command-examples/</guid>
      <description>In this tutorial, we will show you how to use the curl tool through practical examples and detailed explanations of the most common curl options. What is Curl? Curl is command line utility for transferring data from or to a server designed to work without user interaction. With curl you can download or upload data using one of the supported protocols including HTTP, HTTPS, SCP, SFTP and&amp;hellip;</description>
    </item>
    
    <item>
      <title>Day 67-81</title>
      <link>/post/day-6781/</link>
      <pubDate>Sun, 15 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/day-6781/</guid>
      <description>The motivation to face the fear is similarly straightforward: my R code runs too slowly for some of the problems I care about. It doesn’t come up that often, to be honest. Most problems I work on are small enough that it really doesn’t matter that my R code is slow. Other problems are standard enough that I can rely on other people’s compiled code to do all the work (e.</description>
    </item>
    
    <item>
      <title>how should I get started with R?</title>
      <link>/post/how-should-i-get-started-with-/</link>
      <pubDate>Sun, 15 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/how-should-i-get-started-with-/</guid>
      <description>Here’s some evergreen advice from David Robinson: Many of the folks I talk to about learning R have little or no experience with “real” programming languages, which described myself when I first installed the language. If you’re in this camp, I have a few recommendations to get started. The second thing I’d suggest is starting a GitHub account, and to begin curating some of your&amp;hellip;</description>
    </item>
    
    <item>
      <title>Generate a reproducible map for county-level fertilizer estimation data in U.S.A. using R</title>
      <link>/post/generate-a-reproducible-map-fo/</link>
      <pubDate>Fri, 13 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/generate-a-reproducible-map-fo/</guid>
      <description>More than 70% of researchers have tried and failed to reproduce another scientist’s experiments, and more than half have failed to reproduce their own experiments. There are also some packages required to reproduce this post. If you have not installed them, please run the following codes. After installing all the libraries, we should include them in the R session to run the following codes. The map actually is a ggplot2 object and users can modified most of the components using ggplot2&amp;hellip;</description>
    </item>
    
    <item>
      <title>Linear Models</title>
      <link>/post/linear-models/</link>
      <pubDate>Fri, 06 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/linear-models/</guid>
      <description>Introduction library(tidyverse) library(matlib) library(knitr) library(RColorBrewer) The purpose of this document is to understand the parameter and residuals error estimates in a basic linear regression model when working with binary categorical variables. Recall the general model definition: [ \mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{e}] where (\mathbf{X}) is the design matrix and (\mathbf{\beta}) is a ((p+1))-vector of coefficients/parameters, including the intercept&amp;hellip;</description>
    </item>
    
    <item>
      <title>Wide data to long using the tidyverse (tidyr&#39;s gather function)</title>
      <link>/post/wide-data-to-long-using-the-ti/</link>
      <pubDate>Fri, 06 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/wide-data-to-long-using-the-ti/</guid>
      <description>A wide data storage format is an efficient and compact way to store information. And this organization perhaps it makes data easier to inspect. We have wide monitors our laptops and destops. However, for visualization and analysis you generally need to transform this data from the wide format to a “tidy”, long format. We look at the case where just one variable is stored in a&amp;hellip;</description>
    </item>
    
    <item>
      <title>Bayesian Multilevel Model with Missing Data</title>
      <link>/post/bayesian-multilevel-model-with/</link>
      <pubDate>Thu, 05 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-multilevel-model-with/</guid>
      <description>This is the first post in a three-part blog series I am putting together. The focus of this initial post is effective exploration of the reasons for missingness in a particular set of data. The second post in the series will focus on running and evaluating the imputation model itself after having identified the appropriate covariates that help account for&amp;hellip;</description>
    </item>
    
    <item>
      <title>Bayesian Baby Steps</title>
      <link>/post/bayesian-baby-steps/</link>
      <pubDate>Wed, 04 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-baby-steps/</guid>
      <description>Before going into the regression example with a predictor, it’s worthwhile to first demonstrate quadratic approximation by just modeling the score differential with a Gaussian. First step is to visualize the score differential distribution: Not bad, but then again this was a pretty easy&amp;hellip;</description>
    </item>
    
    <item>
      <title>Re-referencing factor levels to estimate standard errors when there is interaction turns out to be a really simple solution</title>
      <link>/post/rereferencing-factor-levels-to/</link>
      <pubDate>Tue, 26 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/rereferencing-factor-levels-to/</guid>
      <description>Maybe this should be filed under topics that are so obvious that it is not worth writing about. But, I hate to let a good simulation just sit on my computer. I was recently working on a paper investigating the relationship of emotion knowledge (EK) in very young kids with academic performance a year or two later. The idea is that kids who are more emotionally intelligent might be better prepared to&amp;hellip;</description>
    </item>
    
    <item>
      <title>Selection effects</title>
      <link>/post/selection-effects/</link>
      <pubDate>Mon, 25 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/selection-effects/</guid>
      <description>My limited goals: Perhaps the central difference between working in the Stata environment and in R is that in R you always have to be declaring which data frame you are working with. In Stata, you just have one active data frame and then you can refer to the variables by their names alone. The tidyverse tools with piping make working in R feel more like working in Stata in my&amp;hellip;</description>
    </item>
    
    <item>
      <title>Wget Command Examples</title>
      <link>/post/wget-command-examples/</link>
      <pubDate>Tue, 19 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/wget-command-examples/</guid>
      <description>In this tutorial, we will show you how to use the Wget utility through practical examples and detailed explanations of the most common Wget options. What is Wget? GNU Wget is command line utility for downloading files from the web. With Wget you can download files using HTTP, HTTPS, and FTP protocols. Wget provides a number of options allowing you to download multiple files, resume downloads, limit the bandwidth, recursive downloads, download in a background, mirror a website and much&amp;hellip;</description>
    </item>
    
    <item>
      <title>Multivariate Adaptive Regression Splines in a Nutshell</title>
      <link>/post/multivariate-adaptive-regressi/</link>
      <pubDate>Sat, 16 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/multivariate-adaptive-regressi/</guid>
      <description>Like standard linear regression, MARS uses the ordinary least squares (OLS) method to estimate the coefficient of each term. However, instead of an original predictor, each term in a MARS model is a basis function derived from original predictors. A basis function takes one of the following forms: MARS does not treat categorical predictors differently from standard linear&amp;hellip;</description>
    </item>
    
    <item>
      <title>How to Find Files in Linux Using the Command Line</title>
      <link>/post/how-to-find-files-in-linux-usi/</link>
      <pubDate>Wed, 13 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/how-to-find-files-in-linux-usi/</guid>
      <description>Find is a command-line utility which allows you to search for files and directories in a directory hierarchy based on a user given expression and applies a user specified action on each matched file. The Linux Find command is one of the most powerful tools in the Linux system administrators arsenal. You can use the find command to search for files based on the file permissions, type, date, ownership, size and&amp;hellip;</description>
    </item>
    
    <item>
      <title>Late anniversary edition redux</title>
      <link>/post/late-anniversary-edition-redux/</link>
      <pubDate>Wed, 13 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/late-anniversary-edition-redux/</guid>
      <description>This afternoon, I was looking over some simulations I plan to use in an upcoming lecture on multilevel models. I created these examples a while ago, before I started this blog. But since it was just about a year ago that I first wrote about this topic (and started the blog), I thought I’d post this now to mark the occasion. As the variation across clusters increases, so does the discrepancy between the conditional and marginal&amp;hellip;</description>
    </item>
    
    <item>
      <title>tiler 0.2.0 CRAN release</title>
      <link>/post/tiler-020-cran-release/</link>
      <pubDate>Mon, 11 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tiler-020-cran-release/</guid>
      <description>Lastly, consider the power of your system before attempting to make a ton of tiles for large images at very high resolutions. You could find that the system could hang at any one of a number of choke points. If you are attempting to make thousands of tiles for a large, high resolution image and your system is struggling, it is recommended to (1) try making tiles for only one zoom level at a time, starting from zero and then increasing while monitoring your system&amp;hellip;</description>
    </item>
    
    <item>
      <title>Could an Independent Yorkshire Win the World Cup - Rest of the World/UK</title>
      <link>/post/could-an-independent-yorkshire/</link>
      <pubDate>Sun, 10 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/could-an-independent-yorkshire/</guid>
      <description>To save time, I’m gonig to used saved versions of the datasets I built up over the 5 blog posts. I won’t include the functions in this blog post either, but the article uses (at most very slight modified) functions from the previous 5 posts. We first need to sort the players into either the UK vs. the rest of the World* and finding the optimal teams for each, as we did&amp;hellip;</description>
    </item>
    
    <item>
      <title>When &#39;Docker&#39; meets &#39;Make&#39;</title>
      <link>/post/when-docker-meets-make/</link>
      <pubDate>Sun, 10 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/when-docker-meets-make/</guid>
      <description>Being a DevOps engineer, it&amp;rsquo;s very common that we use tools like AWS CLI, Docker/ECS, and Ansible to build continuous deployment solutions. It is also common to use tools like JenkinsCI to fully automate the deployment of your applications. Recently I have experienced that, due to some bizarre and varied reasons, you cannot always use CI. By removing CI from the picture, we introduce other issues (e.g. you cannot run the deployment scripts as-is on any&amp;hellip;</description>
    </item>
    
    <item>
      <title>Could an Independent Yorkshire Win the World Cup - Simulate World Cups</title>
      <link>/post/could-an-independent-yorkshire/</link>
      <pubDate>Sat, 09 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/could-an-independent-yorkshire/</guid>
      <description>Now that we have the teams for each county, we want to work out how well they would do at a world cup. For this, we need to know roughly what their ranking would be compared to actual nations. Two sources of rankings of nations are the official FIFA world rankings, and also the world ELO ratings of each nation at www.eloratings.net. I scraped both of these (accurate to mid-May) and cleaned the data to match the nation names to those in the player dataset we’re&amp;hellip;</description>
    </item>
    
    <item>
      <title>What’s the IGO dataset?</title>
      <link>/post/whats-the-igo-dataset/</link>
      <pubDate>Fri, 08 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/whats-the-igo-dataset/</guid>
      <description>This webpage is meant provide students and the curious with an visual, explorable introduction to the dataset. The number of IGOs observed over the time period of 1815 to 2005 has dramatically increased. At the beginning of this period there were just a handful, but now they number more than 300. The number of IGO-country memberships, likewise has also grown dramatically. Individual country affiliations with IGOs numbers more than&amp;hellip;</description>
    </item>
    
    <item>
      <title>Article Round Up June 2018</title>
      <link>/post/article-round-up-june-2018/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/article-round-up-june-2018/</guid>
      <description>The first article is quite long, but easily skim-able. It focuses on not the super wealthy but the elite professionals that make up the upper class and the pernicious ways that group has convinced itself that membership is meritocratic when in reality parental wealth is inherited to a high&amp;hellip;</description>
    </item>
    
    <item>
      <title>Could an Independent Yorkshire Win the World Cup - LASSOs and Player Positions</title>
      <link>/post/could-an-independent-yorkshire/</link>
      <pubDate>Wed, 06 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/could-an-independent-yorkshire/</guid>
      <description>The data we’ve scraped only gives a player’s overall ‘ability’ and their abilities on specific skills (e.g. strength, long shots, dribbling…). We want to use this to work out how good each player is at each position. This positional ability score is important as we can’t just select the 11 best players for each team as we might end up playing a goalkeeper and 10 defenders (or etc.). We need to make sure we select the best palyers for each position on a realistic&amp;hellip;</description>
    </item>
    
    <item>
      <title>A Brief Introduction to Bagged Trees, Random Forest and Their Applications in R</title>
      <link>/post/a-brief-introduction-to-bagged/</link>
      <pubDate>Tue, 05 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-brief-introduction-to-bagged/</guid>
      <description>It should be noted that although the bagged trees are identically distributed, they are not necessarily independent. Since the boostrap samples used to train each individual tree come from the same data set, it is not surprising that the trees may share some similar structure. This similarity, known as tree correlation, is an essential factor that prevents further reduction of variance (hence further improvement of performance) for bagged&amp;hellip;</description>
    </item>
    
    <item>
      <title>Day 38: Algorithmic complexity</title>
      <link>/post/day-38-algorithmic-complexity/</link>
      <pubDate>Sun, 03 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/day-38-algorithmic-complexity/</guid>
      <description>can be produced with a very short R program: whereas a random-looking string like However, if I use R as the compressing language, there is a very short program that produces it: All of which is by way of background. And calling it… Not surprisingly, complexity increases as a sequence becomes longer, even if it’s the the same symbol being&amp;hellip;</description>
    </item>
    
    <item>
      <title>A note on factors in regression (in R)</title>
      <link>/post/a-note-on-factors-in-regressio/</link>
      <pubDate>Sat, 02 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-note-on-factors-in-regressio/</guid>
      <description>Factors terrify me. I can avoid dealing with them most of the time, but they’re immensely useful in a regression when you have a categorical variable with many levels (e.g. “Very Bad”, “Bad”, “Good”, “Very Good”). A common example in economics is when you have a difference-in-differences design and you want to estimate how the treatment effect changes over time, before and after the treatment&amp;hellip;</description>
    </item>
    
    <item>
      <title>Day 36-37: Concerned DALEX</title>
      <link>/post/day-3637-concerned-dalex/</link>
      <pubDate>Sat, 02 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/day-3637-concerned-dalex/</guid>
      <description>I was working on a longer post continuing the metaprogramming series, and realised I wasn’t going to get it done this evening. But it’s been a couple of days since I tried out something new, so I resorted to the twitters to find inspiration. As always, the wonderful twitter rstats folks rose to the occasion: If I’ve understood this correctly, what the figure is showing me is what happens to the model prediction as I add the predictors in one by&amp;hellip;</description>
    </item>
    
    <item>
      <title>Bayesian Baby Steps</title>
      <link>/post/bayesian-baby-steps/</link>
      <pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-baby-steps/</guid>
      <description>You’re trying to evaluate a receiver’s ability to catch a football. Let’s pretend you can take the following (completely unrealistic) strategy: you tell your quarterback to repeatedly throw the ball to your receiver in practice, recording each time whether or not they caught the ball. We’ll let C stand for catch and D stand for&amp;hellip;</description>
    </item>
    
    <item>
      <title>My favourite snippets</title>
      <link>/post/my-favourite-snippets/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/my-favourite-snippets/</guid>
      <description>A hidden gem from Rstudio is snippets feature. A well known option in any other editor (Atom, VS Code, Notepad ++&amp;hellip;.) seems that for R people is not a very used tool. For what I know some developers tend to code a full Add-in for things that can be achieved easily just adding a snippet to your Rstudio configuration. Doing this is easy. The graphical way is getting to Rstudio Tools &amp;gt; Global Options &amp;gt; Code &amp;gt; Enable Code Snippets (Edit&amp;hellip;</description>
    </item>
    
    <item>
      <title>Jenkins as a Service</title>
      <link>/post/jenkins-as-a-service/</link>
      <pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/jenkins-as-a-service/</guid>
      <description>In this session we will work through provisioning Jenkins on AWS ECS from a set of Docker containers that allow individuals or teams to self service an immutable CI/CD setup. The presentation looks at the 3 levels of the service, Infrastructure automation, Application automation and Job configuration automation. We treat our applications with a strong &amp;lsquo;as code&amp;rsquo; approach, but often forget about the critical operational&amp;hellip;</description>
    </item>
    
    <item>
      <title>Gaussian Process Imputation/Forecast Models</title>
      <link>/post/gaussian-process-imputationfor/</link>
      <pubDate>Mon, 21 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/gaussian-process-imputationfor/</guid>
      <description>As a toy problem, I am going to focus on the application of a Gaussian process model to forecasting future monthly passengers. This is not the only way one could try to solve this prediction problem. I offer it as a means of understanding the potential power that exists in using these sorts of models for prediction and imputation problems involving univariate time series data. First, the data need a bit of prepping to be fed into a Stan program.</description>
    </item>
    
    <item>
      <title>#BoG18: Talk Notes</title>
      <link>/post/bog18-talk-notes/</link>
      <pubDate>Tue, 08 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bog18-talk-notes/</guid>
      <description>Typos everywhere. Things may change dramatically over time as I scan back through notes. I’ve tried to respect #notwitter. Will be updated periodically. Speaker (Last Author) 80+% complete for each of the&amp;hellip;</description>
    </item>
    
    <item>
      <title>Modeling the error variance to account for heteroskedasticity</title>
      <link>/post/modeling-the-error-variance-to/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/modeling-the-error-variance-to/</guid>
      <description>One of the assumptions that comes with applying OLS estimation for regression models in the social sciences is homoskedasticity, I prefer constant error variance (it also goes by spherical disturbances). It implies that there is no systematic pattern to the error variance, meaning the model is equally poor at all levels of prediction. This assumption is important for OLS to be the best linear unbiased predictor&amp;hellip;</description>
    </item>
    
    <item>
      <title>Simulating data from regression models</title>
      <link>/post/simulating-data-from-regressio/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/simulating-data-from-regressio/</guid>
      <description>My preferred approach to validating regression models is to simulate data from them, and see if the simulated data capture relevant features of the original data. A basic feature of interest would be the mean. I like this approach because it is extendable to the family of generalized linear models (logistic, Poisson, gamma, &amp;hellip;) and other regression models, say t-regression. It&amp;rsquo;s something Gelman and Hill cover in their regression&amp;hellip;</description>
    </item>
    
    <item>
      <title>Writing an R Package Basics (and why I think you should)</title>
      <link>/post/writing-an-r-package-basics-an/</link>
      <pubDate>Tue, 24 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/writing-an-r-package-basics-an/</guid>
      <description>On April 10, 2018, I gave a talk entitled Developing your first R package: A case study with esvis for the Eugene R Users Group. Although I discussed my esvis package, the focus of the talk was really on tools and tips for developing R pacakges. In this post, I&amp;rsquo;ll go over some of the content in that talk, and discuss why I think you should develop an R&amp;hellip;</description>
    </item>
    
    <item>
      <title>Testing multiple interventions in a single experiment</title>
      <link>/post/testing-multiple-interventions/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/testing-multiple-interventions/</guid>
      <description>First, a bit about multi-factorial data. A single factor is a categorical variable that can have any number of levels. In this context, the factor is usually describing some level of intervention or exposure. As an example, if we want to expose some material to one of three temperature settings, the variable would take on the values “cold”, “moderate”, or&amp;hellip;</description>
    </item>
    
    <item>
      <title>How to install Java on Debian 9</title>
      <link>/post/how-to-install-java-on-debian-/</link>
      <pubDate>Mon, 16 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/how-to-install-java-on-debian-/</guid>
      <description>In this tutorial, we will walk through installing Java on Debian 9. Java is one of the most popular programming languages used to build different kinds of applications and systems. Applications developed in Java are scalable, flexible and maintainable. There are two different Java packages, Java Runtime Environment (JRE) and the Java Development Kit&amp;hellip;</description>
    </item>
    
    <item>
      <title>Predicting NFL Injuries with Stan</title>
      <link>/post/predicting-nfl-injuries-with-s/</link>
      <pubDate>Sun, 15 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/predicting-nfl-injuries-with-s/</guid>
      <description>Yesterday I wrote a post using Stan to fit simple one parameter models. These are boring, but helpful for learning the basics. Today, I’d like to start building a series of increasingly complicated regression models. I have data on all the injuries that occured during the 2017 NFL (American Football) season, courtesy of&amp;hellip;</description>
    </item>
    
    <item>
      <title>Stan Basics</title>
      <link>/post/stan-basics/</link>
      <pubDate>Sat, 14 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/stan-basics/</guid>
      <description>I attended a great short course on bayesian workflow using Stan at the New England Statistics Symposium yesterday. If you don’t know, Stan is “a state-of-the-art platform for statistical modeling and high-performance statistical computation”. You can easily interface with Stan through R (or python or a bunch of other languages). I figured it would be valuable for myself, and possibly others, to work through a few different problems in Stan and share my&amp;hellip;</description>
    </item>
    
    <item>
      <title>YMMV: non-profit data science</title>
      <link>/post/ymmv-nonprofit-data-science/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ymmv-nonprofit-data-science/</guid>
      <description>Feeling inspired by some recent data science collaborations, on Friday I released the following tweet into the wild: Publicly it seemed to garner a good deal of positive attention, although I did also receive some valid criticism via DMs. All of this combined got me thinking about the best way to address sharing (and building) your data science skills by volunteering with a non-profit organization&amp;hellip;</description>
    </item>
    
    <item>
      <title>California School Dashboards Part 3</title>
      <link>/post/california-school-dashboards-p/</link>
      <pubDate>Sat, 31 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/california-school-dashboards-p/</guid>
      <description>This is part three of a three part series where I work with California School Dashboard data by cleaning, visualizaing, and exploring through modeling. You can read the first part of this series, which shows one way to clean and prepare the data, and the second part of the series, which shows a way to visualize the&amp;hellip;</description>
    </item>
    
    <item>
      <title>California School Dashboards Part 2</title>
      <link>/post/california-school-dashboards-p/</link>
      <pubDate>Sun, 18 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/california-school-dashboards-p/</guid>
      <description>This is part two of a three part series where I’ll be working with California School Dashboard data by cleaning, visualizaing, and exploring through modeling. You can read the first part of this series, which shows one way to clean and prepare the data, at this&amp;hellip;</description>
    </item>
    
    <item>
      <title>Functional programming in R with Purrr</title>
      <link>/post/functional-programming-in-r-wi/</link>
      <pubDate>Sun, 18 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/functional-programming-in-r-wi/</guid>
      <description>When you first started in R you likely were writing simple code to generate one outcome. This is great, you are learning about strings, math, and vectors in R! Then you get started with some basic analyses. You want to see if you can find the mean of some&amp;hellip;</description>
    </item>
    
    <item>
      <title>A gentle guide to Tidy statistics in R</title>
      <link>/post/a-gentle-guide-to-tidy-statist/</link>
      <pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-gentle-guide-to-tidy-statist/</guid>
      <description>We will be using MMSE (mini-mental status exam) scores to assess the degree of cognitive impairment. In a real clinical trial, many other variables would be recorded, but for the sake of a straightforward but multi-variate example we will stick to just MMSE. We will be working through loading, plotting, analyzing, and saving the outputs of our analysis through the tidyverse, an “opinionated collection of R packages” designed for data&amp;hellip;</description>
    </item>
    
    <item>
      <title>EPL Week 30</title>
      <link>/post/epl-week-30/</link>
      <pubDate>Wed, 14 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/epl-week-30/</guid>
      <description>For the remainder of the season, I will be travelling with a back up laptop so please excuse any shortfall in posts and site updates Match of the DayRashford schools Alex-Arnold Palace joined WBA with a league-leading ninth one-goal defeat. Every team has suffered at least one such occurrence this season. Surprisingly Huddersfield have lost by a single goal just once in their 15 defeatsManager changesSeveral teams have changed manager’s this season in an attempt to acquire a ‘New manger&amp;hellip;</description>
    </item>
    
    <item>
      <title>Interpretable Machine Learning with iml and mlr</title>
      <link>/post/interpretable-machine-learning/</link>
      <pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/interpretable-machine-learning/</guid>
      <description>Machine learning models repeatedly outperform interpretable, parametric models like the linear regression model. The gains in performance have a price: The models operate as black boxes which are not interpretable. Fortunately, there are many methods that can make machine learning models&amp;hellip;</description>
    </item>
    
    <item>
      <title>Training Courses for mlr</title>
      <link>/post/training-courses-for-mlr/</link>
      <pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/training-courses-for-mlr/</guid>
      <description>The mlr: Machine Learning in R package provides a generic, object-oriented and extensible framework for classification, regression, survival analysis and clustering for the statistical programming language R. The package targets practitioners who want to quickly apply machine learning algorithms, as well as researchers who want to implement, benchmark, and compare their new methods in a structured environment. We are happy to announce that we now offer training courses specialized on&amp;hellip;</description>
    </item>
    
    <item>
      <title>Fundamentos de Inferencia Estadística</title>
      <link>/post/fundamentos-de-inferencia-esta/</link>
      <pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/fundamentos-de-inferencia-esta/</guid>
      <description>A pesar de que en la formación en psicología se nos ofrecen varios cursos sobre estadística descriptiva e inferencial, difícilmente los estudiantes comprenden a qué se refiere exactamente el tema. Es común, relacionar la inferencia con la aplicación de pruebas estadísticas (e.g. anova, prueba t, correlaciones), sin entender realmente las ideas centrales. Dado que no soy estadístico, probablemente hayan imprecisiones, en todo caso me asumo responsable y los invito a leer de manera crítica el&amp;hellip;</description>
    </item>
    
    <item>
      <title>It&#39;s Showtime!</title>
      <link>/post/its-showtime/</link>
      <pubDate>Mon, 26 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/its-showtime/</guid>
      <description>Benefits of putting on a Show(case) I&amp;rsquo;ve found regular showcases one of the most effective tools in the Agile bag of tricks. Here are some of the things I&amp;rsquo;ve used showcases for: Sharing your work Well this one is pretty obvious. However sometimes teams don&amp;rsquo;t think the work they do is showcaseable, or that only &amp;lsquo;customer facing&amp;rsquo; applications should be showcased. I believe that if you are taking a product based approach to your work (which you should&amp;hellip;</description>
    </item>
    
    <item>
      <title>Brief introduction of storm hysteresis effects in solute concentration-stream discharge (C-Q) relationship</title>
      <link>/post/brief-introduction-of-storm-hy/</link>
      <pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/brief-introduction-of-storm-hy/</guid>
      <description>Generally, in order to investigate the dynamics of stream discharge and solute concentrations (C-Q relationship) in a watershed, researchers and environmental engineers usually set up monitoring stations in the watershed outlet. As the temporal dynamics of discharge and solute concentration is an integration of complex hydrological and biochemical processes, the relationship between solute concentration and discharge, hereafter called C-Q relationship, is mostly&amp;hellip;</description>
    </item>
    
    <item>
      <title>A Shiny App to Compare Stats</title>
      <link>/post/a-shiny-app-to-compare-stats/</link>
      <pubDate>Sun, 18 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-shiny-app-to-compare-stats/</guid>
      <description>For a recent publication comparing null hypothesis testing p-values to Bayes Factors and Observation Oriented Modeling, we created a Shiny app to graph all of our complex plots. I particularly pleased with the plotly 3D graph - as I usually think that 3D graphs are impossible to read. This plot shows what we found in our study (albeit I would recommend viewing the 2D plots more): Bayes Factors and p-values follow a power function, as we&amp;hellip;</description>
    </item>
    
    <item>
      <title>Covariate Adjustment for Binary Outcomes in Randomized Trials</title>
      <link>/post/covariate-adjustment-for-binar/</link>
      <pubDate>Sun, 18 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/covariate-adjustment-for-binar/</guid>
      <description>Introduction A common misconception about randomized clinical trials is that the randomization process should balance any particular covariate across the arms of the trial and that therefore there is no benefit to controlling for covariates with a regression model unless a particular covariate happens to be unbalanced by&amp;hellip;</description>
    </item>
    
    <item>
      <title>Research Statement</title>
      <link>/post/research-statement/</link>
      <pubDate>Sat, 17 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/research-statement/</guid>
      <description>Upon arriving at Missouri State University, I founded the Deciphering Outrageous Observations and Modeling (DOOM) lab which has included more than ten graduate and thirty undergraduate students. My research mission has been in two primary domains described in detail below and includes many collaborative efforts throughout the years. Psycholinguistics. My cognitive research focuses broadly on psycholinguistics and memory, particularly on the statistical properties of word&amp;hellip;</description>
    </item>
    
    <item>
      <title>Teaching Luxembourgish to my computer</title>
      <link>/post/teaching-luxembourgish-to-my-c/</link>
      <pubDate>Wed, 14 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/teaching-luxembourgish-to-my-c/</guid>
      <description>Today we reveal a project that Kevin and myself have been working on for the past 2 months, Liss. Liss is a sentiment analysis artificial intelligence; you can let Liss read single words or whole sentences, and Liss will tell you if the overall sentiment is either positive or negative. Such tools are used in marketing, to determine how people perceive a certain brand or new products for instance. The originality of Liss, is that it works on Luxembourguish&amp;hellip;</description>
    </item>
    
    <item>
      <title>Using binary regression software to model ordinal data as a multivariate GLM</title>
      <link>/post/using-binary-regression-softwa/</link>
      <pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/using-binary-regression-softwa/</guid>
      <description>I have read that the most common model for analyzing ordinal data is the cumulative link logistic model, coupled with the proportional odds assumption. Essentially, you treat the outcome as if it were the categorical manifestation of a continuous latent variable. The predictor variables of this outcome influence it in one way only, so you get a single regression coefficient for each&amp;hellip;</description>
    </item>
    
    <item>
      <title>California School Dashboards Part 1</title>
      <link>/post/california-school-dashboards-p/</link>
      <pubDate>Thu, 08 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/california-school-dashboards-p/</guid>
      <description>This is part one of a three part series where I’ll be working with California School Dashboard data by cleaning, visualizaing, and exploring through modeling. Introduction: It’s Ok to Skip Around I’m writing this series for data scientists, public school educators, and data scientists who are also public school&amp;hellip;</description>
    </item>
    
    <item>
      <title>Flagging toxic comments with Tidytext and Keras</title>
      <link>/post/flagging-toxic-comments-with-t/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/flagging-toxic-comments-with-t/</guid>
      <description>The task here is to try to determine how likely a string is to have a particular set of labels. We can take a look at the data The first step for looking at the actual text is to split up the strings into words and then remove stop words. Stop words like “the” and “a” are unlikely to provide much information about the toxicity of the comment, so we can take them out.</description>
    </item>
    
    <item>
      <title>Have you ever asked yourself, &#39;how should I approach the classic pre-post analysis?&#39;</title>
      <link>/post/have-you-ever-asked-yourself-h/</link>
      <pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/have-you-ever-asked-yourself-h/</guid>
      <description>I’ve explored various scenarios (i.e. different data generating assumptions) to see if it matters which approach we use. (Of course it does.) The plots show the three different types of analysis - follow-up measurement alone, change, or follow-up controlling for baseline: I compare the different modeling approaches by using simulation to estimate statistical power for each. That is, given that there is some effect, how often is the p-value of the test less than&amp;hellip;</description>
    </item>
    
    <item>
      <title>Linear Models</title>
      <link>/post/linear-models/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/linear-models/</guid>
      <description>Preamble The purpose of this post is to elucidate some of the concepts associated with statistical linear models. Let’s start by loading some libraries. library(ggplot2) library(datasets) Background Theory The basic idea is as follows: Given two variables, (x) and (y), for which we’ve measured a set of data points ({x_i, y_i}) with (i = 1, &amp;hellip;, n), we want to estimate a function, (f(x)), such that [y_i = f(x_i) +&amp;hellip;</description>
    </item>
    
    <item>
      <title>Importance sampling adds an interesting twist to Monte Carlo simulation</title>
      <link>/post/importance-sampling-adds-an-in/</link>
      <pubDate>Thu, 18 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/importance-sampling-adds-an-in/</guid>
      <description>Like many of the topics I’ve written about, this is a vast one that certainly warrants much, much more than a blog entry. MC simulation in particular, since it is so fundamental to the practice of statistics. MC methods are an essential tool to understand the behavior of statistical models. In fact, I’ve probably used MC simulations in just about all of my posts - to generate repeated samples from a model to explore bias, variance, and other distributional characteristics of a particular&amp;hellip;</description>
    </item>
    
    <item>
      <title>Simulating a cost-effectiveness analysis to highlight new functions for generating correlated data</title>
      <link>/post/simulating-a-costeffectiveness/</link>
      <pubDate>Mon, 08 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/simulating-a-costeffectiveness/</guid>
      <description>In the simulation scenario I’ve concocted, the goal is to increase the number of patients that come in for an important test. A group of public health professionals have developed a new outreach program that they think will be able to draw in more patients. The study is conducted at the site level - some sites will implement the new approach, and the others, serving as controls, will continue with the existing approach.</description>
    </item>
    
    <item>
      <title>Churn Analysis - Part 1</title>
      <link>/post/churn-analysis-part-1/</link>
      <pubDate>Thu, 04 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/churn-analysis-part-1/</guid>
      <description>Hello everyone, We can shortly define customer churn (most commonly called “churn”) as customers that stop doing business with a company or a service. There are customer churns in different business area. In this post, we will focus on the telecom area. Here, we want to predict which customers will leave their current telecom provider. This type of studies are called churn&amp;hellip;</description>
    </item>
    
    <item>
      <title>Using glmer() to perform Rasch analysis</title>
      <link>/post/using-glmer-to-perform-rasch-a/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/using-glmer-to-perform-rasch-a/</guid>
      <description>I&amp;rsquo;ve been interested in the relationship between ordinal regression and item response theory (IRT) for a few months now. There are several helpful papers on the topic, here are some randomly picked ones 1 2 3 4 5, and a book.6 In this post, I focus on Rasch&amp;hellip;</description>
    </item>
    
    <item>
      <title>Post-selection inference on Friends titles in R</title>
      <link>/post/postselection-inference-on-fri/</link>
      <pubDate>Fri, 22 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/postselection-inference-on-fri/</guid>
      <description>Goal I want to be a Friends scriptwriter. Can I pick a title that makes an episode an automatic classic? If I just include a character’s name in the title, does it make it automatically popular? I assume I should just write “The One Where Rachel is Rachel”. But let’s&amp;hellip;</description>
    </item>
    
    <item>
      <title>When there&#39;s a fork in the road, take it. Or, taking a look at marginal structural models.</title>
      <link>/post/when-theres-a-fork-in-the-road/</link>
      <pubDate>Mon, 11 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/when-theres-a-fork-in-the-road/</guid>
      <description>The DAG below is a simple version of how things can get complicated very fast if we have sequential treatments or exposures that both affect and are affected by intermediate factors or conditions. In reality, there are no parallel universes. Maybe we could come up with an actual randomized experiment to mimic this, but it may be difficult. More likely, we’ll have observed data that looks like this: Alternatively, we can just generate the observed data&amp;hellip;</description>
    </item>
    
    <item>
      <title>Writing a paper with RStudio</title>
      <link>/post/writing-a-paper-with-rstudio/</link>
      <pubDate>Sat, 09 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/writing-a-paper-with-rstudio/</guid>
      <description>This semester I had to write a paper for my Financial Econometrics class. My topic was on analyzing the volatility of Bitcoin using GARCH modeling. I’m not particularly interested in Bitcoin, but with all the recent news around it, and with its highly volatile characteristics, I figured it would be a good candidate for analysis. I did the analysis in R, but I wanted to take it a step further. Could I write the entire paper in R and RStudio in a fairly professional format?</description>
    </item>
    
    <item>
      <title>When you use inverse probability weighting for estimation, what are the weights actually doing?</title>
      <link>/post/when-you-use-inverse-probabili/</link>
      <pubDate>Mon, 04 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/when-you-use-inverse-probabili/</guid>
      <description>Because binary outcomes are less amenable to visual illustration, I am going to stick with model estimation to see how this plays out: We can look directly at the potential outcomes to see the true causal effect, measured as a log odds ratio (LOR): In the linear regression context, the conditional effect measured using observed data from the exposed and unexposed subgroups was in fact a good estimate of the marginal effect in the&amp;hellip;</description>
    </item>
    
    <item>
      <title>Visualizing how confounding biases estimates of population-wide (or marginal) average causal effects</title>
      <link>/post/visualizing-how-confounding-bi/</link>
      <pubDate>Thu, 16 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/visualizing-how-confounding-bi/</guid>
      <description>When we are trying to assess the effect of an exposure or intervention on an outcome, confounding is an ever-present threat to our ability to draw the proper conclusions. My goal (starting here and continuing in upcoming posts) is to think a bit about how to characterize confounding in a way that makes it possible to literally see why improperly estimating intervention effects might lead to bias. Unfortunately, in the real world, it is rarely feasible to expose an individual to multiple&amp;hellip;</description>
    </item>
    
    <item>
      <title>3 Keys for Successful Products and Programs Before You Even Start</title>
      <link>/post/3-keys-for-successful-products/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/3-keys-for-successful-products/</guid>
      <description>If you’ve been an information security practitioner for more than a few years, you’ve likely witnessed your share of disappointing purchases, implementations and initiatives. If you’ve been charged with managing multiple information security projects, you might have experienced a torrent of&amp;hellip;</description>
    </item>
    
    <item>
      <title>Mapping Statewide School Ratings with US Census Tracts</title>
      <link>/post/mapping-statewide-school-ratin/</link>
      <pubDate>Mon, 13 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/mapping-statewide-school-ratin/</guid>
      <description>In this post, I&amp;rsquo;d like to share some work related to geo-spatial mapping, statewide school ratings, and US Census Bureau data using census tracts. Specifically, I wanted to investigate whether there was a relation between the median housing price in an area, and the statewide achievement ratings for schools in the corresponding&amp;hellip;</description>
    </item>
    
    <item>
      <title>Firearms Sourced and Recovered in the United States and Territories 2010-2016</title>
      <link>/post/firearms-sourced-and-recovered/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/firearms-sourced-and-recovered/</guid>
      <description>I want to try and probe a question that was raised since Las Vegas and now revived due to the tragedy in Sutherland Springs,TX: Given the free trade between states, can a state unilaterally regulate firearms. This post will try to start to give an answer to this question using R. The shiny app can be run directly We read in the data sources In the app it is easy to see that since the Sandy Hook mass shooting high regulation has caused a large net inflow of firearms into&amp;hellip;</description>
    </item>
    
    <item>
      <title>esvis: Part 1</title>
      <link>/post/esvis-part-1/</link>
      <pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/esvis-part-1/</guid>
      <description>This is the first of a series of posts to introduce my new esvis R package, why I think it&amp;rsquo;s important, and some of its capabilities. As of this writing the current version on CRAN is 0.0.1, so it&amp;rsquo;s obviously still fresh and may have some bugs. If you find any, please let me know. You can install the package like you would any other on R install.packages(&amp;ldquo;esvis&amp;rdquo;) or if you&amp;rsquo;d prefer the sometimes buggy but more feature-heavy development version, install from github with&amp;hellip;</description>
    </item>
    
    <item>
      <title>Misspecification and fit indices in covariance-based SEM</title>
      <link>/post/misspecification-and-fit-indic/</link>
      <pubDate>Sat, 28 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/misspecification-and-fit-indic/</guid>
      <description>TLDR: If you have good measurement quality, conventional benchmarks for fit indices may lead to bad decisions. Additionally, global fit indices are not informative for investigating misspecification. I am working with one of my professors, Dr. Jessica Logan, on a checklist for the developmental progress of young children. We intend to take this down the IRT route (or ordinal logistic regression), but currently, this is all part of a factor analysis course&amp;hellip;</description>
    </item>
    
    <item>
      <title>Who knew likelihood functions could be so pretty?</title>
      <link>/post/who-knew-likelihood-functions-/</link>
      <pubDate>Mon, 23 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/who-knew-likelihood-functions-/</guid>
      <description>We are generally most interested in finding out where the peak of that curve is, because the parameters associated with that point (the maximum likelihood estimates) are often used to describe the “true” underlying data generating process. However, we are also quite interested in the shape of the likelihood curve itself, because that provides information about how certain we can be about our conclusions about the “true”&amp;hellip;</description>
    </item>
    
    <item>
      <title>Use CircleCI for R Projects</title>
      <link>/post/use-circleci-for-r-projects/</link>
      <pubDate>Wed, 18 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/use-circleci-for-r-projects/</guid>
      <description>Why CircleCI? Yes, I know using Travis CI is this easy, thanks to devtools package: Travis CI is OK most of the time. Still, CircleCI has some advantages: Though Travis can cache the setup once it succeeds, it is good if we can save time to setup testing environment by using existing Docker images. CircleCI displays the test summary in this pretty way: All I had to do was two steps.</description>
    </item>
    
    <item>
      <title>Bayesian Estimation of Signal Detection Models, Part 3</title>
      <link>/post/bayesian-estimation-of-signal-/</link>
      <pubDate>Mon, 16 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-estimation-of-signal-/</guid>
      <description>This post is the third part in a series of blog posts on Signal Detection models: In the first part, I described how to estimate the equal variance Gaussian SDT (EVSDT) model for a single participant, using Bayesian (generalized linear and nonlinear) modeling techniques. In the second part, I described how to estimate the equal variance Gaussian SDT model for multiple participants simultaneously, using hierarchical Bayesian&amp;hellip;</description>
    </item>
    
    <item>
      <title>Can we use B-splines to generate non-linear data?</title>
      <link>/post/can-we-use-bsplines-to-generat/</link>
      <pubDate>Mon, 16 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/can-we-use-bsplines-to-generat/</guid>
      <description>Within a cut-point region, the sum of the basis functions always equals 1. This is easy to see by looking at a plot of basis functions, several of which are provided below. The definition and shape of the basis functions do not in any way depend on the data, only on the degree and cut-points. Of course, these functions can be added together in infinitely different ways using weights. If one is trying to fit a B-spline line to data, those weights can be estimated using regression&amp;hellip;</description>
    </item>
    
    <item>
      <title>Bayesian Estimation of Signal Detection Models, Part 2</title>
      <link>/post/bayesian-estimation-of-signal-/</link>
      <pubDate>Thu, 12 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-estimation-of-signal-/</guid>
      <description>This post is the second part of a series of three blog posts: In the first part, I described how to estimate the equal variance Gaussian SDT (EVSDT) model for a single participant, using Bayesian (generalized linear and nonlinear) modeling techniques. In the third part, I describe how to estimate the unequal variance Gaussian SDT model as a nonlinear Bayesian&amp;hellip;</description>
    </item>
    
    <item>
      <title>Bayesian Estimation of Signal Detection Models, Part 1</title>
      <link>/post/bayesian-estimation-of-signal-/</link>
      <pubDate>Mon, 09 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-estimation-of-signal-/</guid>
      <description>This post is the first part of a series of three blog posts: In the second part, I describe how to estimate the equal variance Gaussian SDT model for multiple participants simultaneously, using hierarchical Bayesian models. In the third part, I describe how to estimate the unequal variance Gaussian SDT model as a hierarchical nonlinear Bayesian&amp;hellip;</description>
    </item>
    
    <item>
      <title>An Introduction to Web Performance Optimisation</title>
      <link>/post/an-introduction-to-web-perform/</link>
      <pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/an-introduction-to-web-perform/</guid>
      <description>This is an excerpt from a talk I gave at LASTConf Melbourne on Web Performance Optimisation. In this short video I discuss some of the benefits that web optimisation can bring to internal web pages, and what this means for our concentration and stress levels. In the full talk, (video, slides), I cover: The psychology of performance The benefits of optimisation Why we get it wrong What to measure Content optimisations Delivery optimisations The key web performance optimisation&amp;hellip;</description>
    </item>
    
    <item>
      <title>A minor update to simstudy provides an excuse to talk a bit about the negative binomial and Poisson distributions</title>
      <link>/post/a-minor-update-to-simstudy-pro/</link>
      <pubDate>Thu, 05 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/a-minor-update-to-simstudy-pro/</guid>
      <description>As part of the release, I thought I’d explore the negative binomial just a bit, particularly as it relates to the Poisson distribution. The Poisson distribution is a discrete (integer) distribution of outcomes of non-negative values that is often used to describe count outcomes. It is characterized by a mean (or rate) and its variance equals its mean. We can see this by generating data from each distribution with mean 15, and a dispersion parameter of 0.</description>
    </item>
    
    <item>
      <title>Little&#39;s MCAR test at different sample sizes</title>
      <link>/post/littles-mcar-test-at-different/</link>
      <pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/littles-mcar-test-at-different/</guid>
      <description>TLDR: Little&amp;rsquo;s MCAR test is unable to tell data that are MCAR from data that are MAR in small samples, but maintains the nominal error rate when null is true across a wide range of sample sizes. I just found out about the R simglm package and decided to do a small simulation to test Little&amp;rsquo;s MCAR test1 under different sample sizes. I could have investigated heteroskedasticity in linear regression instead, and I probably will in the&amp;hellip;</description>
    </item>
    
    <item>
      <title>Theil-Sen regression in R</title>
      <link>/post/theilsen-regression-in-r/</link>
      <pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/theilsen-regression-in-r/</guid>
      <description>TLDR: When performing a simple linear regression, if you have any concern about outliers or heterosedasticity, consider the Theil-Sen estimator. A simple linear regression estimator that is not commonly used or taught in the social sciences is the Theil-Sen estimator. This is a shame given that this estimator is very intuitive, once you know what a slope&amp;hellip;</description>
    </item>
    
    <item>
      <title>A simstudy update provides an excuse to talk a little bit about latent class regression and the EM algorithm</title>
      <link>/post/a-simstudy-update-provides-an-/</link>
      <pubDate>Wed, 20 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/a-simstudy-update-provides-an-/</guid>
      <description>It is probably easiest to see in action: Here is what the data look like: Here is a slow-motion version of the EM estimation process. I show the parameter estimates (visually) at the early stages of estimation, checking in after every three steps. In addition, I highlight two individuals and show the estimated probabilities of cluster membership. At the beginning, there is little differentiation between the regression lines for each&amp;hellip;</description>
    </item>
    
    <item>
      <title>Linear regression with violation of heteroskedasticity with small samples</title>
      <link>/post/linear-regression-with-violati/</link>
      <pubDate>Tue, 19 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/linear-regression-with-violati/</guid>
      <description>TLDR: In small samples, the wild bootstrap implemented in the R hcci package is a good bet when heteroskedasticity is a concern. Today while teaching the multiple regression lab, I showed the class the standardized residuals versus standardized predictor plot SPSS lets you produce. It is the plot we typically use to assess homoskedasticity. The sample size for the analysis was 44. I mentioned how the regression slopes are fine under heteroskedasticity, but inference $(t, SE, pvalue)$ may be&amp;hellip;</description>
    </item>
    
    <item>
      <title>Pur(r)ify Your Carets</title>
      <link>/post/purrify-your-carets/</link>
      <pubDate>Sun, 17 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/purrify-your-carets/</guid>
      <description>The motivation An example using BostonHousing data Load libs &amp;amp; data Create a starter dataframe Select the models Create data-model combinations Solve the models Extract results In conclusion tl;dr: You’ll learn how to use purrr, caret and list-cols to quickly create hundreds of dataset + model combinations, store data &amp;amp; model objects neatly in one tibble, and post process programatically. These tools enable succinct functional programming in which a lot gets done with just a few lines of&amp;hellip;</description>
    </item>
    
    <item>
      <title>Advice for non-traditional data scientists</title>
      <link>/post/advice-for-nontraditional-data/</link>
      <pubDate>Tue, 29 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/advice-for-nontraditional-data/</guid>
      <description>I have a pretty strange background for a data scientist. In my career I’ve sold electric razors, worked on credit derivatives during the 2008 financial crash, written market reports on orthopaedic biomaterials, and practiced law. I started programming in R during law school, partly as a way to learn more about data visualization and partly to help analyze youth criminal justice data. Over time I came to enjoy programming more than law and decided to make the switch to data work about three years&amp;hellip;</description>
    </item>
    
    <item>
      <title>A hidden process behind binary or other categorical outcomes?</title>
      <link>/post/a-hidden-process-behind-binary/</link>
      <pubDate>Mon, 28 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/a-hidden-process-behind-binary/</guid>
      <description>I was thinking a lot about proportional-odds cumulative logit models last fall while designing a study to evaluate an intervention’s effect on meat consumption. After a fairly extensive pilot study, we had determined that participants can have quite a difficult time recalling precise quantities of meat consumption, so we were forced to move to a categorical&amp;hellip;</description>
    </item>
    
    <item>
      <title>OSGeo-Live Project</title>
      <link>/post/osgeolive-project/</link>
      <pubDate>Thu, 24 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/osgeolive-project/</guid>
      <description>OSGeo-Live is a self-containing ISO with around 50 FOSS dedicated to GIS. A virtual macine archive is also provided. I join the project in November (or early December) 2015, as I was already a librist when I was a became a GIS technician. As I started a Bachelor degree in GIS, I need GIS tools to do assingnements, and they often needed FOSS like QGIS and/or PostgreSQL/PostGIS, or a&amp;hellip;</description>
    </item>
    
    <item>
      <title>purrr Tricks with All Subset Regression</title>
      <link>/post/purrr-tricks-with-all-subset-r/</link>
      <pubDate>Thu, 24 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/purrr-tricks-with-all-subset-r/</guid>
      <description>All Subsets Regression What is all subsets regression? It’s a technique for model building which involves taking a set of independent variables (X1..i) and regressing them in sets of (k), where (k) is in ({1,2,\dots,i}), against the response variable (Y). The ‘all’ part of ‘all subsets’ means it’s every combination of (X{1..i}) being drawn (k) at a&amp;hellip;</description>
    </item>
    
    <item>
      <title>Be careful not to control for a post-exposure covariate</title>
      <link>/post/be-careful-not-to-control-for-/</link>
      <pubDate>Mon, 21 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/be-careful-not-to-control-for-/</guid>
      <description>The field of causal inference is a rich one, and I won’t even scratch the surface here. My goal is to present the concepts of potential outcomes so that we can articulate at least one clear way to think about what a causal effect can be defined. Under this framework, we generate data where we can find out the “true” measure of causal effect. And then we can use simple regression models to see how well (or not) they recapture these “known” causal&amp;hellip;</description>
    </item>
    
    <item>
      <title>Angular on Electron, part 2</title>
      <link>/post/angular-on-electron-part-2/</link>
      <pubDate>Sun, 20 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/angular-on-electron-part-2/</guid>
      <description>In the previous post the bootstrap of Angular project on Electron platform was described. In this one, the process of application packaging will be presented. One of the benefits of Electron is that it runs on all major platforms. Each platform has naturally its needs regarding to creating the distribution package. Fortunately, the packaging and all that stuff do not need to be done manually. There are few tools which will help you.</description>
    </item>
    
    <item>
      <title>Which RStudio blog posts “pleased” Hadley? A tidytext &#43; web scraping analysis</title>
      <link>/post/which-rstudio-blog-posts-pleas/</link>
      <pubDate>Wed, 16 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/which-rstudio-blog-posts-pleas/</guid>
      <description>Awhile back, I saw a conversation on twitter about how Hadley uses the word “pleased” very often when introducing a new blog post (I couldn’t seem to find this tweet anymore. Can anyone help?). Out of curiousity, and to flex my R web scraping muscles a bit, I’ve decided to analyze the 240+ blog posts that RStudio has put out since 2011. This post will do a few things: Spoiler alert: Hadley uses “pleased”&amp;hellip;</description>
    </item>
    
    <item>
      <title>On the interpretation of regression coefficients</title>
      <link>/post/on-the-interpretation-of-regre/</link>
      <pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/on-the-interpretation-of-regre/</guid>
      <description>TLDR: We should interpret regression coefficients for continuous variables as we would descriptive dummy variables, unless we intend to make causal claims. I am going to be teaching regression labs in the Fall, and somehow, I stumbled onto Gelman and Hill&amp;rsquo;s Data analysis using regression and multilevel/hierarchical models.1 So I started reading it and it&amp;rsquo;s a good book. A useful piece of advice they give is to interpret regression coefficients in a predictive manner&amp;hellip;</description>
    </item>
    
    <item>
      <title>Making this Blog</title>
      <link>/post/making-this-blog/</link>
      <pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/making-this-blog/</guid>
      <description>blogdown magic I made this blog using the magic of blogdown which is a framework that joins together a few technologies to make ‘static websites’ RMarkdown Hugo RMarkdown RMarkdown is an interface from R to markdown which is a way of marking up a text document with formating so other tools can convert it from a text file to a pretty format (PDF, Word, etc.) It’s a great way of documenting your code by having your narrative right next to the code that generated the&amp;hellip;</description>
    </item>
    
    <item>
      <title>Combining R and Python for data analysis</title>
      <link>/post/combining-r-and-python-for-dat/</link>
      <pubDate>Fri, 04 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/combining-r-and-python-for-dat/</guid>
      <description>The main drawback of doing it this way is that I am losing on the interactive explorative tools included in HyperSpy. For this reason my workflow has been to interactively explore and develop the Python code in a Jupyter Notebook, and then copy the final script to a Python chunk in an R Markdown document. I do this to keep the final product together and get a completely reproducible analysis in my R Markdown document.</description>
    </item>
    
    <item>
      <title>janitor, a good R package for Data Wrangling</title>
      <link>/post/janitor-a-good-r-package-for-d/</link>
      <pubDate>Thu, 27 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/janitor-a-good-r-package-for-d/</guid>
      <description>We all know the many hours spent cleaning and wrangling data. Sometimes I think my actual job is not “Data Scientist” but “Data Cleaner”. Data, as you surely know, is not often in the best shape, so for many people like me, one of the most appreciated tools is the one that makes cleaning&amp;hellip;</description>
    </item>
    
    <item>
      <title>Correlated Psychological Variables, Uncertainty, and Bayesian Estimation</title>
      <link>/post/correlated-psychological-varia/</link>
      <pubDate>Tue, 18 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/correlated-psychological-varia/</guid>
      <description>Assessing the correlations between psychological variabless, such as abilities and improvements, is one essential goal of psychological science. However, psychological variables are usually only available to the researcher as estimated parameters in mathematical and statistical models. The parameters are often estimated from small samples of observations for each research participant, which results in uncertainty (aka sampling error) about the participant-specific&amp;hellip;</description>
    </item>
    
    <item>
      <title>First look at Tidycensus</title>
      <link>/post/first-look-at-tidycensus/</link>
      <pubDate>Fri, 30 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/first-look-at-tidycensus/</guid>
      <description>The whole future of the US census has been coming under scrutiny recently, but, thankfully, we are getting more tools to scrutinise both its decennial data and that of its sister-source, the American Community service (ACS). Specifically, Kyle Walker’s tidycensus and tigris packages which return data-frames (including shape-files as list-columns, if required) from the census API and Edzer Pebesma’s sf, simple features,&amp;hellip;</description>
    </item>
    
    <item>
      <title>Matrix factorization for recommender systems (part 2)</title>
      <link>/post/matrix-factorization-for-recom/</link>
      <pubDate>Wed, 28 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/matrix-factorization-for-recom/</guid>
      <description>In previous post I explained Weigted Alternating Least Squares algorithm for matrix factorization. This post will be more practical - we will build a model which will recommend artists recommendations based on history of track listenings. Design of evaluation and cross validation Before we will go to modeling we need to discuss how we will validate our&amp;hellip;</description>
    </item>
    
    <item>
      <title>Is Zero-Sum Thinking Affecting Your Risk Decision?</title>
      <link>/post/is-zerosum-thinking-affecting-/</link>
      <pubDate>Tue, 27 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/is-zerosum-thinking-affecting-/</guid>
      <description>One of the challenges we embrace in my line of work is the attempt to identify risk convergence and opportunities for risk reduction across multiple scenarios. It’s not uncommon for these opportunities to cut across business functions or risk assets. When I find these points of convergence I start looking for adjustments that reduce risk exposure and simultaneously improve the customer&amp;hellip;</description>
    </item>
    
    <item>
      <title>Thinking about Workflow</title>
      <link>/post/thinking-about-workflow/</link>
      <pubDate>Thu, 22 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/thinking-about-workflow/</guid>
      <description>In the spring of 2011, I was in the middle of doing research for my dissertation. I had recently returned from my second extended trip to the archives in the Netherlands and Belgium and had accumulated a ton of notes. I knew that technology had drastically altered the possibilities for research, but the fundamentals of my own workflow were hardly different than they had been when I began undergrad in the early&amp;hellip;</description>
    </item>
    
    <item>
      <title>When marginal and conditional logistic model estimates diverge</title>
      <link>/post/when-marginal-and-conditional-/</link>
      <pubDate>Fri, 09 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/when-marginal-and-conditional-/</guid>
      <description>My aim is to show this through a couple of data simulations that allow us to see this visually. Now let’s generate some data and look at it: Since we have repeated measurements for each cluster (the two potential outcomes), we can transform this into a “longitudinal” data set, though the periods are not time but different exposures. When we look at the data visually, we get a hint that the marginal (or average) effect might not be the same as the conditional (cluster-specific)&amp;hellip;</description>
    </item>
    
    <item>
      <title>Tidytext Analysis of Seinfeld</title>
      <link>/post/tidytext-analysis-of-seinfeld/</link>
      <pubDate>Mon, 22 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytext-analysis-of-seinfeld/</guid>
      <description>I then wrote a function that takes the URL for an episode and pulls the necessary data. Unfortunately, as the scripts were submitted to the site by different fans there is no standard format, making the scraping a little trickier. By using a combination of regular expressions and other tools, we are able to pull the necessary&amp;hellip;</description>
    </item>
    
    <item>
      <title>It can be easy to explore data generating mechanisms with the simstudy package</title>
      <link>/post/it-can-be-easy-to-explore-data/</link>
      <pubDate>Tue, 16 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/it-can-be-easy-to-explore-data/</guid>
      <description>I learned statistics and probability by simulating data. Sure, I did the occasional proof, but I never believed the results until I saw it in a simulation. I guess I have it backwards, but I that’s just the way I am. And now that I am a so-called professional, I continue to use simulation to understand models, to do sample size estimates and power calculations, and of course to teach. Sure - I’ll use the occasional formula when one exists, but I always feel the need to check it with&amp;hellip;</description>
    </item>
    
    <item>
      <title>A first taste of the common workflow language, part 1.</title>
      <link>/post/a-first-taste-of-the-common-wo/</link>
      <pubDate>Sun, 07 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/a-first-taste-of-the-common-wo/</guid>
      <description>If you go to the CWL webpage you are greeted, right at the top, by these two sentences: The Common Workflow Language (CWL) is a specification for describing analysis workflows and tools in a way that makes them portable and scalable across a variety of software and hardware environments, from workstations to cluster, cloud, and high performance computing (HPC)&amp;hellip;</description>
    </item>
    
    <item>
      <title>What&#39;s News?</title>
      <link>/post/whats-news/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/whats-news/</guid>
      <description>newsflash packagePolymath, Bob Rudis (aka hrbrmstr) has recently released the newsflash package which is a “set of tools to Work with the Internet Archive and GDELT Television Explorer”&amp;rdquo; In a recent blog post, based on a gdelt project creator article, he details the coverage of Hillary Clinton’s email server woes, with the, unsurprising, fact that FOX News spent more time on the issue than other&amp;hellip;</description>
    </item>
    
    <item>
      <title>Parallel benchmarking with OpenML and mlr</title>
      <link>/post/parallel-benchmarking-with-ope/</link>
      <pubDate>Wed, 22 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/parallel-benchmarking-with-ope/</guid>
      <description>With this post I want to show you how to benchmark several learners (or learners with different parameter settings) using several data sets in a structured and parallelized fashion. For this we want to use batchtools. The data that we will use here is stored on the open machine learning platform openml.org and we can download it together with information on what to do with it in form of a&amp;hellip;</description>
    </item>
    
    <item>
      <title>The Four values of a devops transformation</title>
      <link>/post/the-four-values-of-a-devops-tr/</link>
      <pubDate>Sun, 12 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/the-four-values-of-a-devops-tr/</guid>
      <description>A successful devops transformation sees a change in organisational culture. These changes often come in the way of adoption of specific tools or practices. However, to change culture, you need something more fundamental than just the introduction of new tools, or pushing everyone into Scrum teams. Just like the agile transformations of the past, there was a difference between ‘Doing Agile’, and ‘Being Agile’. ‘We do standups’ - therefore we are&amp;hellip;</description>
    </item>
    
    <item>
      <title>Why?</title>
      <link>/post/why/</link>
      <pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/why/</guid>
      <description>We all know DevOps is not about the tools or the process, it’s about a deeper cultural movement. But all too often we think about what DevOps is, and miss the focus on Why we do it in the first&amp;hellip;</description>
    </item>
    
    <item>
      <title>R for Excel Users</title>
      <link>/post/r-for-excel-users/</link>
      <pubDate>Thu, 02 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/r-for-excel-users/</guid>
      <description>Like most people, I first learned to work with numbers through an Excel spreadsheet. After graduating with an undergraduate philosophy degree, I somehow convinced a medical device marketing firm to give me a job writing Excel reports on the orthopedic biomaterials market. When I first started, I remember not knowing how to anything, but after a few months I became fairly proficient with the tool, and was able to build all sorts of useful&amp;hellip;</description>
    </item>
    
    <item>
      <title>Finite Mixture Modeling using Flexmix</title>
      <link>/post/finite-mixture-modeling-using-/</link>
      <pubDate>Wed, 01 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/finite-mixture-modeling-using-/</guid>
      <description>Model Based Clustering Quick EDA Model building Mixtures of Regressions Quick EDA Model Building Results Further investigation Notes References This page replicates the codes written by Grun &amp;amp; Leish (2007) in ‘FlexMix: An R package for finite mixture modelling’, University of Wollongong, Australia. My intent here was to learn the flexmix package by replicating the results by the authors. Model Based Clustering The model based clustering on the whiskey&amp;hellip;</description>
    </item>
    
    <item>
      <title>Intro to R slides</title>
      <link>/post/intro-to-r-slides/</link>
      <pubDate>Tue, 31 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/intro-to-r-slides/</guid>
      <description>For the Perception Action and Cognition Lab Open Science Week, 2017 (University of Leeds) I gave two talks introducing R. You can see the slides below. The code for the slides can be found over at GitHub. An introduction to R In this introduction to R I focused on tools from the tidyverse, as well as trying to provide some motivation for learning R. The audience was academics and postgraduates in a psychology&amp;hellip;</description>
    </item>
    
    <item>
      <title>Magic reprex</title>
      <link>/post/magic-reprex/</link>
      <pubDate>Wed, 11 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/magic-reprex/</guid>
      <description>Making reproducible examples can be hard. There&amp;rsquo;s a lot of things you need to consider. Like, making sure your environment is clean, the right packages are loaded, the code is formatted nicely, and images are the right resolution and dimension. Getting all of these ducks lined up can sometimes take a couple of minutes, if you have a nice tightly defined problem. Other times, it can take much, much&amp;hellip;</description>
    </item>
    
  </channel>
</rss>